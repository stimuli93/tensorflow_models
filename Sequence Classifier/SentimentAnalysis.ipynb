{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Using LstmClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading Imdb movie review dataset\n",
    "train = pd.read_csv('dataset/labeledTrainData.tsv',quoting=3,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing html content, stopwords, lower-casing from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "stop_words = set(['the','and','a','has','she','of','to','is','had','these','it','in','i','this','that','s','was','as','for','with','t','you','on','he','are','his','have','be','at','they','by','an','there','her','my','we','me','do'])\n",
    "def clean_review(movie_review):\n",
    "    #Removing HTML tags\n",
    "    review = BeautifulSoup(movie_review, \"lxml\").get_text()\n",
    "    \n",
    "    #Removing non alphabetic words\n",
    "    review = re.sub(\"[^a-zA-Z]\",\" \",review)\n",
    "    review = review.lower()\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    cleaned_review = \"\"\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            cleaned_review += \" %s \"%(token)\n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.review = train.review.apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(review):\n",
    "    return len(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1000, 0, 12000]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAECCAYAAAAPX/ubAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+j+snVrZGtD+VwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114459e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wc = train.review.apply(word_count)\n",
    "plt.hist(wc,25)\n",
    "plt.axis([0, 1000, 0, 12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After cleaning most of the reviews have around 100 words so using 100 as max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74175 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in train.review]\n",
    " \n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d unique words tokens.\" % len(word_freq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'movie', 44030),\n",
       " (u'but', 42613),\n",
       " (u'film', 40146),\n",
       " (u'not', 31725),\n",
       " (u'one', 26788),\n",
       " (u'all', 23976),\n",
       " (u'who', 21435),\n",
       " (u'so', 20611),\n",
       " (u'from', 20494),\n",
       " (u'like', 20274),\n",
       " (u'or', 18004),\n",
       " (u'just', 17766),\n",
       " (u'about', 17372),\n",
       " (u'out', 17108),\n",
       " (u'if', 16799),\n",
       " (u'what', 16166),\n",
       " (u'can', 15767),\n",
       " (u'some', 15744),\n",
       " (u'good', 15140),\n",
       " (u'more', 14246),\n",
       " (u'when', 14178),\n",
       " (u'very', 14065),\n",
       " (u'up', 13291),\n",
       " (u'time', 12723),\n",
       " (u'no', 12722),\n",
       " (u'even', 12646),\n",
       " (u'would', 12436),\n",
       " (u'which', 12050),\n",
       " (u'story', 11983),\n",
       " (u'only', 11915),\n",
       " (u'really', 11736),\n",
       " (u'see', 11474),\n",
       " (u'their', 11381),\n",
       " (u'were', 10784),\n",
       " (u'well', 10661),\n",
       " (u'than', 9916),\n",
       " (u'much', 9765),\n",
       " (u'get', 9310),\n",
       " (u'bad', 9301),\n",
       " (u'been', 9286),\n",
       " (u'people', 9285),\n",
       " (u'will', 9209),\n",
       " (u'other', 9160),\n",
       " (u'also', 9155),\n",
       " (u'into', 9109),\n",
       " (u'first', 9061),\n",
       " (u'great', 9058),\n",
       " (u'because', 9045),\n",
       " (u'how', 8897),\n",
       " (u'him', 8878),\n",
       " (u'don', 8843),\n",
       " (u'most', 8784),\n",
       " (u'made', 8362),\n",
       " (u'its', 8177),\n",
       " (u'then', 8117),\n",
       " (u'way', 8026),\n",
       " (u'make', 8021),\n",
       " (u'them', 7970),\n",
       " (u'could', 7921),\n",
       " (u'too', 7827),\n",
       " (u'movies', 7663),\n",
       " (u'any', 7659),\n",
       " (u'after', 7637),\n",
       " (u'think', 7296),\n",
       " (u'characters', 7154),\n",
       " (u'character', 7022),\n",
       " (u'watch', 6972),\n",
       " (u'two', 6906),\n",
       " (u'films', 6887),\n",
       " (u'seen', 6679),\n",
       " (u'many', 6675),\n",
       " (u'life', 6628),\n",
       " (u'being', 6607),\n",
       " (u'plot', 6585),\n",
       " (u'acting', 6490),\n",
       " (u'never', 6484),\n",
       " (u'love', 6453),\n",
       " (u'little', 6435),\n",
       " (u'best', 6414),\n",
       " (u'where', 6390),\n",
       " (u'over', 6331),\n",
       " (u'did', 6296),\n",
       " (u'show', 6294),\n",
       " (u'know', 6166),\n",
       " (u'off', 6028),\n",
       " (u'ever', 5995),\n",
       " (u'man', 5982),\n",
       " (u'does', 5938),\n",
       " (u'here', 5766),\n",
       " (u'better', 5737),\n",
       " (u'your', 5684),\n",
       " (u'end', 5648),\n",
       " (u'still', 5622),\n",
       " (u'say', 5395),\n",
       " (u'scene', 5378),\n",
       " (u'why', 5315),\n",
       " (u'while', 5312),\n",
       " (u'scenes', 5207),\n",
       " (u've', 5182),\n",
       " (u'go', 5156)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of top 100 most common words in the vocabulary with their counts\n",
    "word_freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'opus', 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frequency of 10_000th most common word because we limit the vicabulary to top 10_000 words only\n",
    "word_freq.most_common(10000)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "pad_token = \"PADDING\"\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocab_size- (4+1))\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "index_to_word.append(pad_token)\n",
    "index_to_word.append(sentence_start_token)\n",
    "index_to_word.append(sentence_end_token)\n",
    "\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 100\n",
    "def get_idx_vector(review):\n",
    "    review = '%s %s %s'%(sentence_start_token, review, sentence_end_token)\n",
    "    review_tokens = nltk.word_tokenize(review)\n",
    "    unknown_token_id = word_to_index[unknown_token]\n",
    "    padding_token = word_to_index[pad_token]\n",
    "    idx_vector = np.zeros([1,max_seq_length]) + padding_token\n",
    "    limit = min(len(review_tokens),max_seq_length)\n",
    "    for i in xrange(limit):\n",
    "        idx_vector[0][i] = word_to_index.get(review_tokens[i],unknown_token_id)\n",
    "    return idx_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.99700000e+03,   5.00000000e+00,   4.89000000e+02,\n",
       "          1.29000000e+02,   1.40000000e+02,   5.06000000e+02,\n",
       "          9.03300000e+03,   9.80000000e+01,   5.94000000e+02,\n",
       "          2.56900000e+03,   1.83000000e+02,   1.09000000e+02,\n",
       "          9.68000000e+02,   6.03000000e+02,   8.80000000e+01,\n",
       "          2.53000000e+02,   9.99500000e+03,   2.53000000e+02,\n",
       "          9.99500000e+03,   1.33000000e+02,   2.38000000e+02,\n",
       "          1.10000000e+01,   1.42000000e+02,   3.70000000e+01,\n",
       "          7.54000000e+02,   2.55800000e+03,   4.40000000e+01,\n",
       "          1.85000000e+02,   6.00000000e+00,   1.55000000e+02,\n",
       "          3.00000000e+01,   5.87000000e+02,   4.18200000e+03,\n",
       "          1.10000000e+01,   2.38000000e+02,   5.60000000e+01,\n",
       "          2.20000000e+01,   2.87000000e+02,   6.70000000e+02,\n",
       "          2.44000000e+03,   1.00000000e+01,   1.29100000e+03,\n",
       "          9.99500000e+03,   1.32000000e+02,   4.94100000e+03,\n",
       "          1.32000000e+02,   7.27000000e+02,   2.00000000e+00,\n",
       "          2.70000000e+01,   3.31000000e+02,   1.29000000e+02,\n",
       "          3.10000000e+01,   3.84000000e+02,   2.00000000e+01,\n",
       "          1.76200000e+03,   5.75000000e+02,   1.70000000e+01,\n",
       "          1.24100000e+03,   3.37600000e+03,   1.20000000e+01,\n",
       "          9.03300000e+03,   4.97000000e+02,   8.88000000e+02,\n",
       "          3.45100000e+03,   4.30000000e+01,   5.26000000e+02,\n",
       "          6.90000000e+02,   1.60500000e+03,   3.80000000e+01,\n",
       "          1.01000000e+02,   4.54900000e+03,   1.95300000e+03,\n",
       "          1.09400000e+03,   1.00000000e+00,   2.19000000e+02,\n",
       "          5.00000000e+00,   1.20000000e+01,   4.26000000e+02,\n",
       "          1.54200000e+03,   7.00000000e+00,   8.37000000e+02,\n",
       "          2.52800000e+03,   9.00000000e+00,   9.03300000e+03,\n",
       "          5.04000000e+02,   5.40000000e+01,   1.29000000e+02,\n",
       "          7.30000000e+02,   1.28000000e+02,   3.14000000e+02,\n",
       "          1.70000000e+01,   1.59000000e+02,   6.28000000e+02,\n",
       "          9.03300000e+03,   9.99500000e+03,   9.99500000e+03,\n",
       "          1.88000000e+02,   0.00000000e+00,   1.00000000e+00,\n",
       "          9.03300000e+03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_idx_vector(train.review[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = [get_idx_vector(review) for review in train.review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX = np.array(trX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX = trX.reshape([-1,max_seq_length])\n",
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY = train.sentiment.values\n",
    "trY = pd.get_dummies(trY).values\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(trX, trY, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SequenceClassifier import LstmClassifier\n",
    "clf = LstmClassifier(vocab_size=vocab_size,num_classes=2,max_seq_length=max_seq_length,hidden_size=32,\n",
    "                     embedding_size=32, ckpt_dir=\"./sa_ckpt_dir\", summary_dir=\"/tmp/sa_lstmClf_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sa_ckpt_dir/model.ckpt-3500\n",
      "Loss at step 3500: 0.22449\n",
      "Loss at step 3510: 0.249602\n",
      "Loss at step 3520: 0.125896\n",
      "Loss at step 3530: 0.121497\n",
      "Loss at step 3540: 0.117479\n",
      "Loss at step 3550: 0.157544\n",
      "Loss at step 3560: 0.118991\n",
      "Loss at step 3570: 0.104193\n",
      "Loss at step 3580: 0.112824\n",
      "Loss at step 3590: 0.0897209\n",
      "Loss at step 3600: 0.129032\n",
      "Loss at step 3610: 0.113146\n",
      "Loss at step 3620: 0.179208\n",
      "Loss at step 3630: 0.144841\n",
      "Loss at step 3640: 0.222349\n",
      "Loss at step 3650: 0.121481\n",
      "Loss at step 3660: 0.161337\n",
      "Loss at step 3670: 0.123959\n",
      "Loss at step 3680: 0.272062\n",
      "Loss at step 3690: 0.216111\n",
      "Loss at step 3700: 0.162712\n",
      "Loss at step 3710: 0.0411431\n",
      "Loss at step 3720: 0.126267\n",
      "Loss at step 3730: 0.128701\n",
      "Loss at step 3740: 0.0970572\n",
      "Loss at step 3750: 0.212418\n",
      "Loss at step 3760: 0.186206\n",
      "Loss at step 3770: 0.119732\n",
      "Loss at step 3780: 0.334139\n",
      "Loss at step 3790: 0.0930535\n",
      "Loss at step 3800: 0.296091\n",
      "Loss at step 3810: 0.215721\n",
      "Loss at step 3820: 0.401601\n",
      "Loss at step 3830: 0.255239\n",
      "Loss at step 3840: 0.0905904\n",
      "Loss at step 3850: 0.0846782\n",
      "Loss at step 3860: 0.153535\n",
      "Loss at step 3870: 0.152507\n",
      "Loss at step 3880: 0.274156\n",
      "Loss at step 3890: 0.138043\n",
      "Loss at step 3900: 0.165875\n",
      "Loss at step 3910: 0.128388\n",
      "Loss at step 3920: 0.0679899\n",
      "Loss at step 3930: 0.184371\n",
      "Loss at step 3940: 0.13435\n",
      "Loss at step 3950: 0.218423\n",
      "Loss at step 3960: 0.150774\n",
      "Loss at step 3970: 0.232789\n",
      "Loss at step 3980: 0.118254\n",
      "Loss at step 3990: 0.130369\n",
      "Loss at step 4000: 0.154373\n",
      "Loss at step 4010: 0.0718387\n",
      "Loss at step 4020: 0.138904\n",
      "Loss at step 4030: 0.130089\n",
      "Loss at step 4040: 0.153269\n",
      "Loss at step 4050: 0.176317\n",
      "Loss at step 4060: 0.14033\n",
      "Loss at step 4070: 0.1648\n",
      "Loss at step 4080: 0.138172\n",
      "Loss at step 4090: 0.119544\n",
      "Loss at step 4100: 0.245027\n",
      "Loss at step 4110: 0.140778\n",
      "Loss at step 4120: 0.127378\n",
      "Loss at step 4130: 0.0998255\n",
      "Loss at step 4140: 0.0612446\n",
      "Loss at step 4150: 0.257597\n",
      "Loss at step 4160: 0.12656\n",
      "Loss at step 4170: 0.152535\n",
      "Loss at step 4180: 0.11141\n",
      "Loss at step 4190: 0.15623\n",
      "Loss at step 4200: 0.110022\n",
      "Loss at step 4210: 0.318455\n",
      "Loss at step 4220: 0.147548\n",
      "Loss at step 4230: 0.133314\n",
      "Loss at step 4240: 0.096352\n",
      "Loss at step 4250: 0.183546\n",
      "Loss at step 4260: 0.122895\n",
      "Loss at step 4270: 0.181233\n",
      "Loss at step 4280: 0.134183\n",
      "Loss at step 4290: 0.0776649\n",
      "Loss at step 4300: 0.170488\n",
      "Loss at step 4310: 0.211873\n",
      "Loss at step 4320: 0.179275\n",
      "Loss at step 4330: 0.28044\n",
      "Loss at step 4340: 0.161301\n",
      "Loss at step 4350: 0.197203\n",
      "Loss at step 4360: 0.171253\n",
      "Loss at step 4370: 0.143572\n",
      "Loss at step 4380: 0.159284\n",
      "Loss at step 4390: 0.133437\n",
      "Loss at step 4400: 0.326558\n",
      "Loss at step 4410: 0.188967\n",
      "Loss at step 4420: 0.10273\n",
      "Loss at step 4430: 0.0688708\n",
      "Loss at step 4440: 0.104453\n",
      "Loss at step 4450: 0.178668\n",
      "Loss at step 4460: 0.109265\n",
      "Loss at step 4470: 0.177444\n",
      "Loss at step 4480: 0.108817\n",
      "Loss at step 4490: 0.125283\n",
      "Loss at step 4500: 0.0441733\n",
      "Loss at step 4510: 0.0774783\n",
      "Loss at step 4520: 0.0643477\n",
      "Loss at step 4530: 0.0782608\n",
      "Loss at step 4540: 0.173858\n",
      "Loss at step 4550: 0.214925\n",
      "Loss at step 4560: 0.0800601\n",
      "Loss at step 4570: 0.180133\n",
      "Loss at step 4580: 0.143757\n",
      "Loss at step 4590: 0.0684735\n",
      "Loss at step 4600: 0.236396\n",
      "Loss at step 4610: 0.149967\n",
      "Loss at step 4620: 0.187123\n",
      "Loss at step 4630: 0.113235\n",
      "Loss at step 4640: 0.184444\n",
      "Loss at step 4650: 0.172283\n",
      "Loss at step 4660: 0.0539079\n",
      "Loss at step 4670: 0.116642\n",
      "Loss at step 4680: 0.108046\n",
      "Loss at step 4690: 0.108563\n",
      "Loss at step 4700: 0.106882\n",
      "Loss at step 4710: 0.170519\n",
      "Loss at step 4720: 0.0371235\n",
      "Loss at step 4730: 0.115213\n",
      "Loss at step 4740: 0.143682\n",
      "Loss at step 4750: 0.15919\n",
      "Loss at step 4760: 0.116859\n",
      "Loss at step 4770: 0.12497\n",
      "Loss at step 4780: 0.127938\n",
      "Loss at step 4790: 0.10642\n",
      "Loss at step 4800: 0.103842\n",
      "Loss at step 4810: 0.134867\n",
      "Loss at step 4820: 0.131074\n",
      "Loss at step 4830: 0.127643\n",
      "Loss at step 4840: 0.141315\n",
      "Loss at step 4850: 0.209259\n",
      "Loss at step 4860: 0.16691\n",
      "Loss at step 4870: 0.0953667\n",
      "Loss at step 4880: 0.155469\n",
      "Loss at step 4890: 0.118096\n",
      "Loss at step 4900: 0.121944\n",
      "Loss at step 4910: 0.12608\n",
      "Loss at step 4920: 0.0687951\n",
      "Loss at step 4930: 0.233199\n",
      "Loss at step 4940: 0.232378\n",
      "Loss at step 4950: 0.0799351\n",
      "Loss at step 4960: 0.117007\n",
      "Loss at step 4970: 0.108531\n",
      "Loss at step 4980: 0.081399\n",
      "Loss at step 4990: 0.044452\n"
     ]
    }
   ],
   "source": [
    "clf.train(Xtrain,ytrain,n_iters=1500,learning_rate=3e-4,keep_prob=0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85568   ,  0.38781864])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation set accuracy & loss\n",
    "clf.score(Xval,yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.95664   ,  0.12257269])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train set accuracy & loss\n",
    "clf.score(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
