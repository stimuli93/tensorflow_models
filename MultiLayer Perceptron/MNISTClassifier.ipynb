{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLP class from MultiLayerPerceptron to classify MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = train.drop('label',axis=1).values\n",
    "trX = trX/255.0\n",
    "trY = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY = pd.get_dummies(trY).values\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain,Xval,ytrain,yval = train_test_split(trX,trY,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MultiLayerPerceptron import MLP\n",
    "clf = MLP(input_dim=784, hidden_layers=[300,50], n_classes=10,ckpt_dir=\"./ckpt_dir\", summary_dir=\"/tmp/MLP_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 3.22488\n",
      "Loss at step 10: 2.44478\n",
      "Loss at step 20: 1.87263\n",
      "Loss at step 30: 1.5119\n",
      "Loss at step 40: 1.31113\n",
      "Loss at step 50: 1.14222\n",
      "Loss at step 60: 1.12842\n",
      "Loss at step 70: 0.997101\n",
      "Loss at step 80: 0.982086\n",
      "Loss at step 90: 0.900165\n",
      "Loss at step 100: 0.92931\n",
      "Loss at step 110: 0.868119\n",
      "Loss at step 120: 0.79548\n",
      "Loss at step 130: 0.759539\n",
      "Loss at step 140: 0.792975\n",
      "Loss at step 150: 0.700534\n",
      "Loss at step 160: 0.74628\n",
      "Loss at step 170: 0.668159\n",
      "Loss at step 180: 0.712247\n",
      "Loss at step 190: 0.664714\n",
      "Loss at step 200: 0.599803\n",
      "Loss at step 210: 0.59504\n",
      "Loss at step 220: 0.613304\n",
      "Loss at step 230: 0.58387\n",
      "Loss at step 240: 0.562804\n",
      "Loss at step 250: 0.643464\n",
      "Loss at step 260: 0.510547\n",
      "Loss at step 270: 0.501081\n",
      "Loss at step 280: 0.488945\n",
      "Loss at step 290: 0.497651\n",
      "Loss at step 300: 0.486907\n",
      "Loss at step 310: 0.470096\n",
      "Loss at step 320: 0.447592\n",
      "Loss at step 330: 0.430841\n",
      "Loss at step 340: 0.439461\n",
      "Loss at step 350: 0.456813\n",
      "Loss at step 360: 0.416238\n",
      "Loss at step 370: 0.386195\n",
      "Loss at step 380: 0.46528\n",
      "Loss at step 390: 0.411393\n",
      "Loss at step 400: 0.405908\n",
      "Loss at step 410: 0.43138\n",
      "Loss at step 420: 0.391499\n",
      "Loss at step 430: 0.359621\n",
      "Loss at step 440: 0.374872\n",
      "Loss at step 450: 0.365961\n",
      "Loss at step 460: 0.386606\n",
      "Loss at step 470: 0.335493\n",
      "Loss at step 480: 0.362237\n",
      "Loss at step 490: 0.355482\n",
      "Loss at step 500: 0.302145\n",
      "Loss at step 510: 0.341761\n",
      "Loss at step 520: 0.348117\n",
      "Loss at step 530: 0.333965\n",
      "Loss at step 540: 0.316937\n",
      "Loss at step 550: 0.342016\n",
      "Loss at step 560: 0.320016\n",
      "Loss at step 570: 0.275133\n",
      "Loss at step 580: 0.339114\n",
      "Loss at step 590: 0.296842\n",
      "Loss at step 600: 0.304918\n",
      "Loss at step 610: 0.318776\n",
      "Loss at step 620: 0.310118\n",
      "Loss at step 630: 0.306221\n",
      "Loss at step 640: 0.283976\n",
      "Loss at step 650: 0.292196\n",
      "Loss at step 660: 0.261812\n",
      "Loss at step 670: 0.26047\n",
      "Loss at step 680: 0.290096\n",
      "Loss at step 690: 0.296012\n",
      "Loss at step 700: 0.298257\n",
      "Loss at step 710: 0.260741\n",
      "Loss at step 720: 0.295294\n",
      "Loss at step 730: 0.254059\n",
      "Loss at step 740: 0.256593\n",
      "Loss at step 750: 0.288976\n",
      "Loss at step 760: 0.259389\n",
      "Loss at step 770: 0.243165\n",
      "Loss at step 780: 0.261541\n",
      "Loss at step 790: 0.243948\n",
      "Loss at step 800: 0.277828\n",
      "Loss at step 810: 0.232833\n",
      "Loss at step 820: 0.247836\n",
      "Loss at step 830: 0.227653\n",
      "Loss at step 840: 0.251082\n",
      "Loss at step 850: 0.238215\n",
      "Loss at step 860: 0.244384\n",
      "Loss at step 870: 0.246637\n",
      "Loss at step 880: 0.236609\n",
      "Loss at step 890: 0.230071\n",
      "Loss at step 900: 0.206602\n",
      "Loss at step 910: 0.202977\n",
      "Loss at step 920: 0.22055\n",
      "Loss at step 930: 0.233012\n",
      "Loss at step 940: 0.19476\n",
      "Loss at step 950: 0.1997\n",
      "Loss at step 960: 0.210608\n",
      "Loss at step 970: 0.216522\n",
      "Loss at step 980: 0.233264\n",
      "Loss at step 990: 0.208026\n",
      "Loss at step 1000: 0.205466\n",
      "Loss at step 1010: 0.198006\n",
      "Loss at step 1020: 0.196216\n",
      "Loss at step 1030: 0.203146\n",
      "Loss at step 1040: 0.188563\n",
      "Loss at step 1050: 0.186934\n",
      "Loss at step 1060: 0.193401\n",
      "Loss at step 1070: 0.193419\n",
      "Loss at step 1080: 0.2094\n",
      "Loss at step 1090: 0.185655\n",
      "Loss at step 1100: 0.190522\n",
      "Loss at step 1110: 0.187696\n",
      "Loss at step 1120: 0.185509\n",
      "Loss at step 1130: 0.182905\n",
      "Loss at step 1140: 0.176044\n",
      "Loss at step 1150: 0.202914\n",
      "Loss at step 1160: 0.177009\n",
      "Loss at step 1170: 0.183422\n",
      "Loss at step 1180: 0.185076\n",
      "Loss at step 1190: 0.216447\n",
      "Loss at step 1200: 0.183846\n",
      "Loss at step 1210: 0.178278\n",
      "Loss at step 1220: 0.191284\n",
      "Loss at step 1230: 0.162651\n",
      "Loss at step 1240: 0.184809\n",
      "Loss at step 1250: 0.164075\n",
      "Loss at step 1260: 0.175769\n",
      "Loss at step 1270: 0.17003\n",
      "Loss at step 1280: 0.176589\n",
      "Loss at step 1290: 0.163923\n",
      "Loss at step 1300: 0.165313\n",
      "Loss at step 1310: 0.168053\n",
      "Loss at step 1320: 0.16047\n",
      "Loss at step 1330: 0.163468\n",
      "Loss at step 1340: 0.165883\n",
      "Loss at step 1350: 0.163536\n",
      "Loss at step 1360: 0.170563\n",
      "Loss at step 1370: 0.162342\n",
      "Loss at step 1380: 0.163622\n",
      "Loss at step 1390: 0.154442\n",
      "Loss at step 1400: 0.165148\n",
      "Loss at step 1410: 0.157633\n",
      "Loss at step 1420: 0.181016\n",
      "Loss at step 1430: 0.17999\n",
      "Loss at step 1440: 0.161297\n",
      "Loss at step 1450: 0.153615\n",
      "Loss at step 1460: 0.154671\n",
      "Loss at step 1470: 0.148172\n",
      "Loss at step 1480: 0.15171\n",
      "Loss at step 1490: 0.15738\n",
      "Loss at step 1500: 0.150944\n",
      "Loss at step 1510: 0.174014\n",
      "Loss at step 1520: 0.16093\n",
      "Loss at step 1530: 0.157499\n",
      "Loss at step 1540: 0.168607\n",
      "Loss at step 1550: 0.161922\n",
      "Loss at step 1560: 0.163668\n",
      "Loss at step 1570: 0.156073\n",
      "Loss at step 1580: 0.144381\n",
      "Loss at step 1590: 0.144254\n",
      "Loss at step 1600: 0.144909\n",
      "Loss at step 1610: 0.152979\n",
      "Loss at step 1620: 0.150001\n",
      "Loss at step 1630: 0.149609\n",
      "Loss at step 1640: 0.149373\n",
      "Loss at step 1650: 0.150819\n",
      "Loss at step 1660: 0.145263\n",
      "Loss at step 1670: 0.167734\n",
      "Loss at step 1680: 0.14578\n",
      "Loss at step 1690: 0.149924\n",
      "Loss at step 1700: 0.163857\n",
      "Loss at step 1710: 0.169766\n",
      "Loss at step 1720: 0.141438\n",
      "Loss at step 1730: 0.142063\n",
      "Loss at step 1740: 0.135322\n",
      "Loss at step 1750: 0.136868\n",
      "Loss at step 1760: 0.135539\n",
      "Loss at step 1770: 0.138182\n",
      "Loss at step 1780: 0.158568\n",
      "Loss at step 1790: 0.146264\n",
      "Loss at step 1800: 0.138154\n",
      "Loss at step 1810: 0.128962\n",
      "Loss at step 1820: 0.160383\n",
      "Loss at step 1830: 0.136649\n",
      "Loss at step 1840: 0.124694\n",
      "Loss at step 1850: 0.141559\n",
      "Loss at step 1860: 0.142571\n",
      "Loss at step 1870: 0.148043\n",
      "Loss at step 1880: 0.1547\n",
      "Loss at step 1890: 0.142645\n",
      "Loss at step 1900: 0.14588\n",
      "Loss at step 1910: 0.13526\n",
      "Loss at step 1920: 0.132129\n",
      "Loss at step 1930: 0.141356\n",
      "Loss at step 1940: 0.141427\n",
      "Loss at step 1950: 0.151339\n",
      "Loss at step 1960: 0.128028\n",
      "Loss at step 1970: 0.131903\n",
      "Loss at step 1980: 0.132981\n",
      "Loss at step 1990: 0.137871\n",
      "Loss at step 2000: 0.13071\n",
      "Loss at step 2010: 0.137935\n",
      "Loss at step 2020: 0.138101\n",
      "Loss at step 2030: 0.136534\n",
      "Loss at step 2040: 0.137465\n",
      "Loss at step 2050: 0.128238\n",
      "Loss at step 2060: 0.137888\n",
      "Loss at step 2070: 0.121804\n",
      "Loss at step 2080: 0.138439\n",
      "Loss at step 2090: 0.131912\n",
      "Loss at step 2100: 0.12528\n",
      "Loss at step 2110: 0.122196\n",
      "Loss at step 2120: 0.130739\n",
      "Loss at step 2130: 0.127343\n",
      "Loss at step 2140: 0.13024\n",
      "Loss at step 2150: 0.128511\n",
      "Loss at step 2160: 0.125343\n",
      "Loss at step 2170: 0.121768\n",
      "Loss at step 2180: 0.132987\n",
      "Loss at step 2190: 0.128232\n",
      "Loss at step 2200: 0.130156\n",
      "Loss at step 2210: 0.123416\n",
      "Loss at step 2220: 0.136667\n",
      "Loss at step 2230: 0.11655\n",
      "Loss at step 2240: 0.118134\n",
      "Loss at step 2250: 0.127249\n",
      "Loss at step 2260: 0.147775\n",
      "Loss at step 2270: 0.120889\n",
      "Loss at step 2280: 0.128375\n",
      "Loss at step 2290: 0.126487\n",
      "Loss at step 2300: 0.123478\n",
      "Loss at step 2310: 0.135287\n",
      "Loss at step 2320: 0.130202\n",
      "Loss at step 2330: 0.135373\n",
      "Loss at step 2340: 0.131936\n",
      "Loss at step 2350: 0.130108\n",
      "Loss at step 2360: 0.132538\n",
      "Loss at step 2370: 0.113859\n",
      "Loss at step 2380: 0.127284\n",
      "Loss at step 2390: 0.11724\n",
      "Loss at step 2400: 0.123766\n",
      "Loss at step 2410: 0.114811\n",
      "Loss at step 2420: 0.120774\n",
      "Loss at step 2430: 0.123171\n",
      "Loss at step 2440: 0.123306\n",
      "Loss at step 2450: 0.119272\n",
      "Loss at step 2460: 0.117808\n",
      "Loss at step 2470: 0.130328\n",
      "Loss at step 2480: 0.117596\n",
      "Loss at step 2490: 0.122584\n",
      "Loss at step 2500: 0.124525\n",
      "Loss at step 2510: 0.122347\n",
      "Loss at step 2520: 0.138065\n",
      "Loss at step 2530: 0.138099\n",
      "Loss at step 2540: 0.123635\n",
      "Loss at step 2550: 0.117904\n",
      "Loss at step 2560: 0.11696\n",
      "Loss at step 2570: 0.120226\n",
      "Loss at step 2580: 0.114763\n",
      "Loss at step 2590: 0.108488\n",
      "Loss at step 2600: 0.124544\n",
      "Loss at step 2610: 0.12041\n",
      "Loss at step 2620: 0.109549\n",
      "Loss at step 2630: 0.129752\n",
      "Loss at step 2640: 0.126317\n",
      "Loss at step 2650: 0.113685\n",
      "Loss at step 2660: 0.118605\n",
      "Loss at step 2670: 0.114324\n",
      "Loss at step 2680: 0.117156\n",
      "Loss at step 2690: 0.12921\n",
      "Loss at step 2700: 0.119261\n",
      "Loss at step 2710: 0.117503\n",
      "Loss at step 2720: 0.120369\n",
      "Loss at step 2730: 0.134959\n",
      "Loss at step 2740: 0.108254\n",
      "Loss at step 2750: 0.120035\n",
      "Loss at step 2760: 0.129924\n",
      "Loss at step 2770: 0.113666\n",
      "Loss at step 2780: 0.129957\n",
      "Loss at step 2790: 0.110214\n",
      "Loss at step 2800: 0.112533\n",
      "Loss at step 2810: 0.113212\n",
      "Loss at step 2820: 0.113631\n",
      "Loss at step 2830: 0.120582\n",
      "Loss at step 2840: 0.119368\n",
      "Loss at step 2850: 0.116048\n",
      "Loss at step 2860: 0.124001\n",
      "Loss at step 2870: 0.124741\n",
      "Loss at step 2880: 0.118135\n",
      "Loss at step 2890: 0.113701\n",
      "Loss at step 2900: 0.111265\n",
      "Loss at step 2910: 0.112863\n",
      "Loss at step 2920: 0.118287\n",
      "Loss at step 2930: 0.119593\n",
      "Loss at step 2940: 0.122111\n",
      "Loss at step 2950: 0.11644\n",
      "Loss at step 2960: 0.117529\n",
      "Loss at step 2970: 0.111084\n",
      "Loss at step 2980: 0.109433\n",
      "Loss at step 2990: 0.122353\n",
      "Loss at step 3000: 0.111941\n",
      "Loss at step 3010: 0.116128\n",
      "Loss at step 3020: 0.111787\n",
      "Loss at step 3030: 0.12701\n",
      "Loss at step 3040: 0.112612\n",
      "Loss at step 3050: 0.111958\n",
      "Loss at step 3060: 0.106037\n",
      "Loss at step 3070: 0.114413\n",
      "Loss at step 3080: 0.108305\n",
      "Loss at step 3090: 0.113892\n",
      "Loss at step 3100: 0.113136\n",
      "Loss at step 3110: 0.110363\n",
      "Loss at step 3120: 0.113354\n",
      "Loss at step 3130: 0.121234\n",
      "Loss at step 3140: 0.120438\n",
      "Loss at step 3150: 0.108598\n",
      "Loss at step 3160: 0.109834\n",
      "Loss at step 3170: 0.114616\n",
      "Loss at step 3180: 0.115667\n",
      "Loss at step 3190: 0.116709\n",
      "Loss at step 3200: 0.108599\n",
      "Loss at step 3210: 0.11071\n",
      "Loss at step 3220: 0.115446\n",
      "Loss at step 3230: 0.101513\n",
      "Loss at step 3240: 0.116558\n",
      "Loss at step 3250: 0.112973\n",
      "Loss at step 3260: 0.105906\n",
      "Loss at step 3270: 0.110931\n",
      "Loss at step 3280: 0.12115\n",
      "Loss at step 3290: 0.109672\n",
      "Loss at step 3300: 0.116797\n",
      "Loss at step 3310: 0.108171\n",
      "Loss at step 3320: 0.118303\n",
      "Loss at step 3330: 0.109137\n",
      "Loss at step 3340: 0.112795\n",
      "Loss at step 3350: 0.119662\n",
      "Loss at step 3360: 0.106789\n",
      "Loss at step 3370: 0.106592\n",
      "Loss at step 3380: 0.112633\n",
      "Loss at step 3390: 0.109994\n",
      "Loss at step 3400: 0.110306\n",
      "Loss at step 3410: 0.109864\n",
      "Loss at step 3420: 0.107959\n",
      "Loss at step 3430: 0.121182\n",
      "Loss at step 3440: 0.110767\n",
      "Loss at step 3450: 0.114406\n",
      "Loss at step 3460: 0.103873\n",
      "Loss at step 3470: 0.112679\n",
      "Loss at step 3480: 0.107937\n",
      "Loss at step 3490: 0.108977\n",
      "Loss at step 3500: 0.10953\n",
      "Loss at step 3510: 0.114997\n",
      "Loss at step 3520: 0.123783\n",
      "Loss at step 3530: 0.10401\n",
      "Loss at step 3540: 0.110725\n",
      "Loss at step 3550: 0.113436\n",
      "Loss at step 3560: 0.113547\n",
      "Loss at step 3570: 0.115238\n",
      "Loss at step 3580: 0.111514\n",
      "Loss at step 3590: 0.111834\n",
      "Loss at step 3600: 0.106811\n",
      "Loss at step 3610: 0.116091\n",
      "Loss at step 3620: 0.105671\n",
      "Loss at step 3630: 0.116138\n",
      "Loss at step 3640: 0.110605\n",
      "Loss at step 3650: 0.118128\n",
      "Loss at step 3660: 0.107373\n",
      "Loss at step 3670: 0.101313\n",
      "Loss at step 3680: 0.122014\n",
      "Loss at step 3690: 0.112899\n",
      "Loss at step 3700: 0.109593\n",
      "Loss at step 3710: 0.1043\n",
      "Loss at step 3720: 0.103933\n",
      "Loss at step 3730: 0.101608\n",
      "Loss at step 3740: 0.112834\n",
      "Loss at step 3750: 0.104646\n",
      "Loss at step 3760: 0.100586\n",
      "Loss at step 3770: 0.105176\n",
      "Loss at step 3780: 0.109546\n",
      "Loss at step 3790: 0.110369\n",
      "Loss at step 3800: 0.103991\n",
      "Loss at step 3810: 0.105582\n",
      "Loss at step 3820: 0.106038\n",
      "Loss at step 3830: 0.102786\n",
      "Loss at step 3840: 0.101652\n",
      "Loss at step 3850: 0.110752\n",
      "Loss at step 3860: 0.106256\n",
      "Loss at step 3870: 0.109268\n",
      "Loss at step 3880: 0.106467\n",
      "Loss at step 3890: 0.103638\n",
      "Loss at step 3900: 0.103446\n",
      "Loss at step 3910: 0.112028\n",
      "Loss at step 3920: 0.110206\n",
      "Loss at step 3930: 0.105088\n",
      "Loss at step 3940: 0.102344\n",
      "Loss at step 3950: 0.0994987\n",
      "Loss at step 3960: 0.0982289\n",
      "Loss at step 3970: 0.110464\n",
      "Loss at step 3980: 0.106116\n",
      "Loss at step 3990: 0.111259\n",
      "Loss at step 4000: 0.114649\n",
      "Loss at step 4010: 0.102134\n",
      "Loss at step 4020: 0.105697\n",
      "Loss at step 4030: 0.106987\n",
      "Loss at step 4040: 0.107734\n",
      "Loss at step 4050: 0.101423\n",
      "Loss at step 4060: 0.10298\n",
      "Loss at step 4070: 0.107902\n",
      "Loss at step 4080: 0.10366\n",
      "Loss at step 4090: 0.101728\n",
      "Loss at step 4100: 0.10422\n",
      "Loss at step 4110: 0.0941805\n",
      "Loss at step 4120: 0.11753\n",
      "Loss at step 4130: 0.102099\n",
      "Loss at step 4140: 0.10816\n",
      "Loss at step 4150: 0.105367\n",
      "Loss at step 4160: 0.111508\n",
      "Loss at step 4170: 0.103302\n",
      "Loss at step 4180: 0.105106\n",
      "Loss at step 4190: 0.103308\n",
      "Loss at step 4200: 0.104005\n",
      "Loss at step 4210: 0.102427\n",
      "Loss at step 4220: 0.102224\n",
      "Loss at step 4230: 0.103701\n",
      "Loss at step 4240: 0.107637\n",
      "Loss at step 4250: 0.106628\n",
      "Loss at step 4260: 0.113408\n",
      "Loss at step 4270: 0.109565\n",
      "Loss at step 4280: 0.108516\n",
      "Loss at step 4290: 0.107211\n",
      "Loss at step 4300: 0.103055\n",
      "Loss at step 4310: 0.101886\n",
      "Loss at step 4320: 0.108911\n",
      "Loss at step 4330: 0.106379\n",
      "Loss at step 4340: 0.104495\n",
      "Loss at step 4350: 0.1034\n",
      "Loss at step 4360: 0.104104\n",
      "Loss at step 4370: 0.101436\n",
      "Loss at step 4380: 0.104857\n",
      "Loss at step 4390: 0.108807\n",
      "Loss at step 4400: 0.114413\n",
      "Loss at step 4410: 0.103779\n",
      "Loss at step 4420: 0.103387\n",
      "Loss at step 4430: 0.103199\n",
      "Loss at step 4440: 0.10526\n",
      "Loss at step 4450: 0.107975\n",
      "Loss at step 4460: 0.0977254\n",
      "Loss at step 4470: 0.108694\n",
      "Loss at step 4480: 0.104925\n",
      "Loss at step 4490: 0.102058\n",
      "Loss at step 4500: 0.10031\n",
      "Loss at step 4510: 0.096607\n",
      "Loss at step 4520: 0.0992224\n",
      "Loss at step 4530: 0.101321\n",
      "Loss at step 4540: 0.106617\n",
      "Loss at step 4550: 0.103053\n",
      "Loss at step 4560: 0.110107\n",
      "Loss at step 4570: 0.112263\n",
      "Loss at step 4580: 0.100935\n",
      "Loss at step 4590: 0.0963577\n",
      "Loss at step 4600: 0.102577\n",
      "Loss at step 4610: 0.103085\n",
      "Loss at step 4620: 0.109642\n",
      "Loss at step 4630: 0.102727\n",
      "Loss at step 4640: 0.108483\n",
      "Loss at step 4650: 0.101636\n",
      "Loss at step 4660: 0.102948\n",
      "Loss at step 4670: 0.112196\n",
      "Loss at step 4680: 0.104602\n",
      "Loss at step 4690: 0.112585\n",
      "Loss at step 4700: 0.10626\n",
      "Loss at step 4710: 0.119122\n",
      "Loss at step 4720: 0.100243\n",
      "Loss at step 4730: 0.10135\n",
      "Loss at step 4740: 0.105236\n",
      "Loss at step 4750: 0.104478\n",
      "Loss at step 4760: 0.0982533\n",
      "Loss at step 4770: 0.101072\n",
      "Loss at step 4780: 0.107392\n",
      "Loss at step 4790: 0.101647\n",
      "Loss at step 4800: 0.106486\n",
      "Loss at step 4810: 0.0991452\n",
      "Loss at step 4820: 0.110519\n",
      "Loss at step 4830: 0.103806\n",
      "Loss at step 4840: 0.0982397\n",
      "Loss at step 4850: 0.102715\n",
      "Loss at step 4860: 0.105713\n",
      "Loss at step 4870: 0.112391\n",
      "Loss at step 4880: 0.0967625\n",
      "Loss at step 4890: 0.101555\n",
      "Loss at step 4900: 0.102676\n",
      "Loss at step 4910: 0.102076\n",
      "Loss at step 4920: 0.110513\n",
      "Loss at step 4930: 0.105017\n",
      "Loss at step 4940: 0.102955\n",
      "Loss at step 4950: 0.101883\n",
      "Loss at step 4960: 0.0990145\n",
      "Loss at step 4970: 0.10225\n",
      "Loss at step 4980: 0.108493\n",
      "Loss at step 4990: 0.107953\n",
      "Loss at step 5000: 0.101077\n"
     ]
    }
   ],
   "source": [
    "reg=5e-4\n",
    "clf.train(Xtrain,ytrain,n_iters=5001,batch_size=500,learning_rate=7e-4,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1086949, 0.99508929]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for training data\n",
    "clf.score(Xtrain,ytrain,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.17097664, 0.97321427]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for validation data\n",
    "clf.score(Xval,yval,reg=reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss function and accuracy plots visualized using tensorboard with logdir=MLP_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
