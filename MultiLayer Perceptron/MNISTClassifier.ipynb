{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MLP class from MultiLayerPerceptron to classify MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = train.drop('label',axis=1).values\n",
    "trX = trX/255.0\n",
    "trY = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY = pd.get_dummies(trY).values\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain,Xval,ytrain,yval = train_test_split(trX,trY,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from MultiLayerPerceptron import MLP\n",
    "clf = MLP(input_dim=784, hidden_layers=[300,50], n_classes=10,ckpt_dir=\"./ckpt_dir\", summary_dir=\"/tmp/MLP_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 3.22548\n",
      "Loss at step 10: 2.52713\n",
      "Loss at step 20: 1.89327\n",
      "Loss at step 30: 1.50288\n",
      "Loss at step 40: 1.33039\n",
      "Loss at step 50: 1.16348\n",
      "Loss at step 60: 1.0708\n",
      "Loss at step 70: 1.03709\n",
      "Loss at step 80: 0.96454\n",
      "Loss at step 90: 0.939449\n",
      "Loss at step 100: 0.814062\n",
      "Loss at step 110: 0.904722\n",
      "Loss at step 120: 0.801531\n",
      "Loss at step 130: 0.818343\n",
      "Loss at step 140: 0.71805\n",
      "Loss at step 150: 0.762663\n",
      "Loss at step 160: 0.668608\n",
      "Loss at step 170: 0.719111\n",
      "Loss at step 180: 0.708793\n",
      "Loss at step 190: 0.625058\n",
      "Loss at step 200: 0.603135\n",
      "Loss at step 210: 0.663991\n",
      "Loss at step 220: 0.590169\n",
      "Loss at step 230: 0.58008\n",
      "Loss at step 240: 0.57382\n",
      "Loss at step 250: 0.561045\n",
      "Loss at step 260: 0.561723\n",
      "Loss at step 270: 0.561123\n",
      "Loss at step 280: 0.541985\n",
      "Loss at step 290: 0.506918\n",
      "Loss at step 300: 0.483611\n",
      "Loss at step 310: 0.468118\n",
      "Loss at step 320: 0.457459\n",
      "Loss at step 330: 0.464911\n",
      "Loss at step 340: 0.460073\n",
      "Loss at step 350: 0.463563\n",
      "Loss at step 360: 0.457913\n",
      "Loss at step 370: 0.462668\n",
      "Loss at step 380: 0.41393\n",
      "Loss at step 390: 0.468701\n",
      "Loss at step 400: 0.441041\n",
      "Loss at step 410: 0.377952\n",
      "Loss at step 420: 0.396118\n",
      "Loss at step 430: 0.399792\n",
      "Loss at step 440: 0.367364\n",
      "Loss at step 450: 0.376498\n",
      "Loss at step 460: 0.388333\n",
      "Loss at step 470: 0.37295\n",
      "Loss at step 480: 0.37091\n",
      "Loss at step 490: 0.360026\n",
      "Loss at step 500: 0.378557\n",
      "Loss at step 510: 0.349169\n",
      "Loss at step 520: 0.368374\n",
      "Loss at step 530: 0.328537\n",
      "Loss at step 540: 0.345055\n",
      "Loss at step 550: 0.309693\n",
      "Loss at step 560: 0.328718\n",
      "Loss at step 570: 0.317372\n",
      "Loss at step 580: 0.317316\n",
      "Loss at step 590: 0.325004\n",
      "Loss at step 600: 0.312156\n",
      "Loss at step 610: 0.314012\n",
      "Loss at step 620: 0.312535\n",
      "Loss at step 630: 0.317253\n",
      "Loss at step 640: 0.293376\n",
      "Loss at step 650: 0.31271\n",
      "Loss at step 660: 0.300703\n",
      "Loss at step 670: 0.285882\n",
      "Loss at step 680: 0.328596\n",
      "Loss at step 690: 0.284023\n",
      "Loss at step 700: 0.270449\n",
      "Loss at step 710: 0.251571\n",
      "Loss at step 720: 0.264603\n",
      "Loss at step 730: 0.277791\n",
      "Loss at step 740: 0.245588\n",
      "Loss at step 750: 0.260225\n",
      "Loss at step 760: 0.261801\n",
      "Loss at step 770: 0.259367\n",
      "Loss at step 780: 0.258959\n",
      "Loss at step 790: 0.258507\n",
      "Loss at step 800: 0.242217\n",
      "Loss at step 810: 0.23178\n",
      "Loss at step 820: 0.229488\n",
      "Loss at step 830: 0.246439\n",
      "Loss at step 840: 0.227725\n",
      "Loss at step 850: 0.259057\n",
      "Loss at step 860: 0.229385\n",
      "Loss at step 870: 0.217924\n",
      "Loss at step 880: 0.260085\n",
      "Loss at step 890: 0.237039\n",
      "Loss at step 900: 0.216687\n",
      "Loss at step 910: 0.234829\n",
      "Loss at step 920: 0.210327\n",
      "Loss at step 930: 0.232354\n",
      "Loss at step 940: 0.20954\n",
      "Loss at step 950: 0.213991\n",
      "Loss at step 960: 0.241988\n",
      "Loss at step 970: 0.216526\n",
      "Loss at step 980: 0.2517\n",
      "Loss at step 990: 0.198264\n",
      "Loss at step 1000: 0.223147\n",
      "Loss at step 1010: 0.22567\n",
      "Loss at step 1020: 0.216693\n",
      "Loss at step 1030: 0.20235\n",
      "Loss at step 1040: 0.21382\n",
      "Loss at step 1050: 0.199571\n",
      "Loss at step 1060: 0.219651\n",
      "Loss at step 1070: 0.211834\n",
      "Loss at step 1080: 0.201689\n",
      "Loss at step 1090: 0.208995\n",
      "Loss at step 1100: 0.194134\n",
      "Loss at step 1110: 0.1866\n",
      "Loss at step 1120: 0.200908\n",
      "Loss at step 1130: 0.190135\n",
      "Loss at step 1140: 0.188725\n",
      "Loss at step 1150: 0.198635\n",
      "Loss at step 1160: 0.198247\n",
      "Loss at step 1170: 0.187277\n",
      "Loss at step 1180: 0.18125\n",
      "Loss at step 1190: 0.177025\n",
      "Loss at step 1200: 0.200384\n",
      "Loss at step 1210: 0.185304\n",
      "Loss at step 1220: 0.186805\n",
      "Loss at step 1230: 0.158168\n",
      "Loss at step 1240: 0.186438\n",
      "Loss at step 1250: 0.192564\n",
      "Loss at step 1260: 0.193919\n",
      "Loss at step 1270: 0.172412\n",
      "Loss at step 1280: 0.18442\n",
      "Loss at step 1290: 0.172079\n",
      "Loss at step 1300: 0.174374\n",
      "Loss at step 1310: 0.173565\n",
      "Loss at step 1320: 0.185531\n",
      "Loss at step 1330: 0.175896\n",
      "Loss at step 1340: 0.1642\n",
      "Loss at step 1350: 0.172053\n",
      "Loss at step 1360: 0.157303\n",
      "Loss at step 1370: 0.161517\n",
      "Loss at step 1380: 0.166958\n",
      "Loss at step 1390: 0.161477\n",
      "Loss at step 1400: 0.176637\n",
      "Loss at step 1410: 0.159218\n",
      "Loss at step 1420: 0.150693\n",
      "Loss at step 1430: 0.169711\n",
      "Loss at step 1440: 0.175347\n",
      "Loss at step 1450: 0.179519\n",
      "Loss at step 1460: 0.159964\n",
      "Loss at step 1470: 0.157318\n",
      "Loss at step 1480: 0.155654\n",
      "Loss at step 1490: 0.152715\n",
      "Loss at step 1500: 0.157356\n",
      "Loss at step 1510: 0.15283\n",
      "Loss at step 1520: 0.151374\n",
      "Loss at step 1530: 0.154478\n",
      "Loss at step 1540: 0.163057\n",
      "Loss at step 1550: 0.152013\n",
      "Loss at step 1560: 0.157012\n",
      "Loss at step 1570: 0.154388\n",
      "Loss at step 1580: 0.148876\n",
      "Loss at step 1590: 0.152672\n",
      "Loss at step 1600: 0.164808\n",
      "Loss at step 1610: 0.146498\n",
      "Loss at step 1620: 0.142332\n",
      "Loss at step 1630: 0.164512\n",
      "Loss at step 1640: 0.151362\n",
      "Loss at step 1650: 0.165652\n",
      "Loss at step 1660: 0.145713\n",
      "Loss at step 1670: 0.146166\n",
      "Loss at step 1680: 0.143752\n",
      "Loss at step 1690: 0.148844\n",
      "Loss at step 1700: 0.145401\n",
      "Loss at step 1710: 0.141857\n",
      "Loss at step 1720: 0.149769\n",
      "Loss at step 1730: 0.141866\n",
      "Loss at step 1740: 0.154983\n",
      "Loss at step 1750: 0.15987\n",
      "Loss at step 1760: 0.143664\n",
      "Loss at step 1770: 0.136055\n",
      "Loss at step 1780: 0.13905\n",
      "Loss at step 1790: 0.14939\n",
      "Loss at step 1800: 0.134962\n",
      "Loss at step 1810: 0.146505\n",
      "Loss at step 1820: 0.158291\n",
      "Loss at step 1830: 0.136847\n",
      "Loss at step 1840: 0.132499\n",
      "Loss at step 1850: 0.131562\n",
      "Loss at step 1860: 0.142366\n",
      "Loss at step 1870: 0.142068\n",
      "Loss at step 1880: 0.132529\n",
      "Loss at step 1890: 0.133361\n",
      "Loss at step 1900: 0.149713\n",
      "Loss at step 1910: 0.130944\n",
      "Loss at step 1920: 0.135578\n",
      "Loss at step 1930: 0.13626\n",
      "Loss at step 1940: 0.145432\n",
      "Loss at step 1950: 0.132657\n",
      "Loss at step 1960: 0.136789\n",
      "Loss at step 1970: 0.141062\n",
      "Loss at step 1980: 0.129621\n",
      "Loss at step 1990: 0.140219\n",
      "Loss at step 2000: 0.132013\n",
      "Loss at step 2010: 0.13411\n",
      "Loss at step 2020: 0.130959\n",
      "Loss at step 2030: 0.133341\n",
      "Loss at step 2040: 0.127065\n",
      "Loss at step 2050: 0.128002\n",
      "Loss at step 2060: 0.130407\n",
      "Loss at step 2070: 0.124888\n",
      "Loss at step 2080: 0.129765\n",
      "Loss at step 2090: 0.132273\n",
      "Loss at step 2100: 0.129878\n",
      "Loss at step 2110: 0.140507\n",
      "Loss at step 2120: 0.124974\n",
      "Loss at step 2130: 0.132159\n",
      "Loss at step 2140: 0.131424\n",
      "Loss at step 2150: 0.136925\n",
      "Loss at step 2160: 0.126056\n",
      "Loss at step 2170: 0.130766\n",
      "Loss at step 2180: 0.143813\n",
      "Loss at step 2190: 0.130338\n",
      "Loss at step 2200: 0.130583\n",
      "Loss at step 2210: 0.133229\n",
      "Loss at step 2220: 0.134671\n",
      "Loss at step 2230: 0.136244\n",
      "Loss at step 2240: 0.120833\n",
      "Loss at step 2250: 0.124058\n",
      "Loss at step 2260: 0.132088\n",
      "Loss at step 2270: 0.124095\n",
      "Loss at step 2280: 0.132552\n",
      "Loss at step 2290: 0.132995\n",
      "Loss at step 2300: 0.128551\n",
      "Loss at step 2310: 0.126647\n",
      "Loss at step 2320: 0.126684\n",
      "Loss at step 2330: 0.125833\n",
      "Loss at step 2340: 0.114111\n",
      "Loss at step 2350: 0.128386\n",
      "Loss at step 2360: 0.134998\n",
      "Loss at step 2370: 0.132436\n",
      "Loss at step 2380: 0.125851\n",
      "Loss at step 2390: 0.122441\n",
      "Loss at step 2400: 0.124591\n",
      "Loss at step 2410: 0.137818\n",
      "Loss at step 2420: 0.126129\n",
      "Loss at step 2430: 0.122119\n",
      "Loss at step 2440: 0.12058\n",
      "Loss at step 2450: 0.125724\n",
      "Loss at step 2460: 0.131047\n",
      "Loss at step 2470: 0.132865\n",
      "Loss at step 2480: 0.126998\n",
      "Loss at step 2490: 0.117681\n",
      "Loss at step 2500: 0.112565\n",
      "Loss at step 2510: 0.125503\n",
      "Loss at step 2520: 0.113483\n",
      "Loss at step 2530: 0.13335\n",
      "Loss at step 2540: 0.126442\n",
      "Loss at step 2550: 0.117417\n",
      "Loss at step 2560: 0.127026\n",
      "Loss at step 2570: 0.125327\n",
      "Loss at step 2580: 0.118595\n",
      "Loss at step 2590: 0.134567\n",
      "Loss at step 2600: 0.124748\n",
      "Loss at step 2610: 0.12215\n",
      "Loss at step 2620: 0.115695\n",
      "Loss at step 2630: 0.121666\n",
      "Loss at step 2640: 0.121032\n",
      "Loss at step 2650: 0.115921\n",
      "Loss at step 2660: 0.11754\n",
      "Loss at step 2670: 0.115455\n",
      "Loss at step 2680: 0.111811\n",
      "Loss at step 2690: 0.122747\n",
      "Loss at step 2700: 0.119833\n",
      "Loss at step 2710: 0.120363\n",
      "Loss at step 2720: 0.112886\n",
      "Loss at step 2730: 0.114866\n",
      "Loss at step 2740: 0.116781\n",
      "Loss at step 2750: 0.117926\n",
      "Loss at step 2760: 0.113744\n",
      "Loss at step 2770: 0.118172\n",
      "Loss at step 2780: 0.104534\n",
      "Loss at step 2790: 0.12122\n",
      "Loss at step 2800: 0.116502\n",
      "Loss at step 2810: 0.115139\n",
      "Loss at step 2820: 0.113294\n",
      "Loss at step 2830: 0.11795\n",
      "Loss at step 2840: 0.120597\n",
      "Loss at step 2850: 0.124344\n",
      "Loss at step 2860: 0.114122\n",
      "Loss at step 2870: 0.126276\n",
      "Loss at step 2880: 0.117653\n",
      "Loss at step 2890: 0.118843\n",
      "Loss at step 2900: 0.107944\n",
      "Loss at step 2910: 0.120516\n",
      "Loss at step 2920: 0.12638\n",
      "Loss at step 2930: 0.128294\n",
      "Loss at step 2940: 0.117125\n",
      "Loss at step 2950: 0.115459\n",
      "Loss at step 2960: 0.119708\n",
      "Loss at step 2970: 0.121715\n",
      "Loss at step 2980: 0.116405\n",
      "Loss at step 2990: 0.116174\n",
      "Loss at step 3000: 0.108917\n",
      "Loss at step 3010: 0.116497\n",
      "Loss at step 3020: 0.114657\n",
      "Loss at step 3030: 0.114039\n",
      "Loss at step 3040: 0.115142\n",
      "Loss at step 3050: 0.120646\n",
      "Loss at step 3060: 0.115948\n",
      "Loss at step 3070: 0.122221\n",
      "Loss at step 3080: 0.118369\n",
      "Loss at step 3090: 0.116801\n",
      "Loss at step 3100: 0.115898\n",
      "Loss at step 3110: 0.117131\n",
      "Loss at step 3120: 0.114821\n",
      "Loss at step 3130: 0.116001\n",
      "Loss at step 3140: 0.1099\n",
      "Loss at step 3150: 0.116783\n",
      "Loss at step 3160: 0.121342\n",
      "Loss at step 3170: 0.105285\n",
      "Loss at step 3180: 0.105702\n",
      "Loss at step 3190: 0.113732\n",
      "Loss at step 3200: 0.108516\n",
      "Loss at step 3210: 0.11907\n",
      "Loss at step 3220: 0.116847\n",
      "Loss at step 3230: 0.112345\n",
      "Loss at step 3240: 0.112858\n",
      "Loss at step 3250: 0.11683\n",
      "Loss at step 3260: 0.107174\n",
      "Loss at step 3270: 0.122934\n",
      "Loss at step 3280: 0.110114\n",
      "Loss at step 3290: 0.108688\n",
      "Loss at step 3300: 0.118885\n",
      "Loss at step 3310: 0.110097\n",
      "Loss at step 3320: 0.115889\n",
      "Loss at step 3330: 0.109704\n",
      "Loss at step 3340: 0.113205\n",
      "Loss at step 3350: 0.104965\n",
      "Loss at step 3360: 0.105249\n",
      "Loss at step 3370: 0.116009\n",
      "Loss at step 3380: 0.12292\n",
      "Loss at step 3390: 0.108687\n",
      "Loss at step 3400: 0.112906\n",
      "Loss at step 3410: 0.11202\n",
      "Loss at step 3420: 0.104713\n",
      "Loss at step 3430: 0.110055\n",
      "Loss at step 3440: 0.115409\n",
      "Loss at step 3450: 0.105105\n",
      "Loss at step 3460: 0.105472\n",
      "Loss at step 3470: 0.107809\n",
      "Loss at step 3480: 0.110887\n",
      "Loss at step 3490: 0.109356\n",
      "Loss at step 3500: 0.108366\n",
      "Loss at step 3510: 0.106575\n",
      "Loss at step 3520: 0.104701\n",
      "Loss at step 3530: 0.111209\n",
      "Loss at step 3540: 0.1067\n",
      "Loss at step 3550: 0.114975\n",
      "Loss at step 3560: 0.107477\n",
      "Loss at step 3570: 0.120304\n",
      "Loss at step 3580: 0.118336\n",
      "Loss at step 3590: 0.114459\n",
      "Loss at step 3600: 0.108576\n",
      "Loss at step 3610: 0.112185\n",
      "Loss at step 3620: 0.116492\n",
      "Loss at step 3630: 0.116516\n",
      "Loss at step 3640: 0.108769\n",
      "Loss at step 3650: 0.103135\n",
      "Loss at step 3660: 0.111039\n",
      "Loss at step 3670: 0.103576\n",
      "Loss at step 3680: 0.111461\n",
      "Loss at step 3690: 0.107357\n",
      "Loss at step 3700: 0.103543\n",
      "Loss at step 3710: 0.10405\n",
      "Loss at step 3720: 0.0993827\n",
      "Loss at step 3730: 0.105561\n",
      "Loss at step 3740: 0.108371\n",
      "Loss at step 3750: 0.111691\n",
      "Loss at step 3760: 0.109508\n",
      "Loss at step 3770: 0.114266\n",
      "Loss at step 3780: 0.103858\n",
      "Loss at step 3790: 0.107558\n",
      "Loss at step 3800: 0.102304\n",
      "Loss at step 3810: 0.101619\n",
      "Loss at step 3820: 0.111149\n",
      "Loss at step 3830: 0.105922\n",
      "Loss at step 3840: 0.103189\n",
      "Loss at step 3850: 0.104745\n",
      "Loss at step 3860: 0.102121\n",
      "Loss at step 3870: 0.111586\n",
      "Loss at step 3880: 0.108021\n",
      "Loss at step 3890: 0.109532\n",
      "Loss at step 3900: 0.119295\n",
      "Loss at step 3910: 0.119556\n",
      "Loss at step 3920: 0.115638\n",
      "Loss at step 3930: 0.110275\n",
      "Loss at step 3940: 0.122525\n",
      "Loss at step 3950: 0.108273\n",
      "Loss at step 3960: 0.10723\n",
      "Loss at step 3970: 0.10453\n",
      "Loss at step 3980: 0.114423\n",
      "Loss at step 3990: 0.112314\n",
      "Loss at step 4000: 0.107727\n",
      "Loss at step 4010: 0.116616\n",
      "Loss at step 4020: 0.108567\n",
      "Loss at step 4030: 0.105915\n",
      "Loss at step 4040: 0.109955\n",
      "Loss at step 4050: 0.109926\n",
      "Loss at step 4060: 0.10181\n",
      "Loss at step 4070: 0.102024\n",
      "Loss at step 4080: 0.105355\n",
      "Loss at step 4090: 0.102988\n",
      "Loss at step 4100: 0.105314\n",
      "Loss at step 4110: 0.111333\n",
      "Loss at step 4120: 0.107249\n",
      "Loss at step 4130: 0.104978\n",
      "Loss at step 4140: 0.101688\n",
      "Loss at step 4150: 0.107222\n",
      "Loss at step 4160: 0.109657\n",
      "Loss at step 4170: 0.112524\n",
      "Loss at step 4180: 0.104153\n",
      "Loss at step 4190: 0.105216\n",
      "Loss at step 4200: 0.107577\n",
      "Loss at step 4210: 0.101329\n",
      "Loss at step 4220: 0.110159\n",
      "Loss at step 4230: 0.112123\n",
      "Loss at step 4240: 0.109122\n",
      "Loss at step 4250: 0.109686\n",
      "Loss at step 4260: 0.108255\n",
      "Loss at step 4270: 0.102995\n",
      "Loss at step 4280: 0.114577\n",
      "Loss at step 4290: 0.112137\n",
      "Loss at step 4300: 0.105074\n",
      "Loss at step 4310: 0.109521\n",
      "Loss at step 4320: 0.108081\n",
      "Loss at step 4330: 0.108303\n",
      "Loss at step 4340: 0.105377\n",
      "Loss at step 4350: 0.115384\n",
      "Loss at step 4360: 0.105688\n",
      "Loss at step 4370: 0.102793\n",
      "Loss at step 4380: 0.107295\n",
      "Loss at step 4390: 0.104835\n",
      "Loss at step 4400: 0.101715\n",
      "Loss at step 4410: 0.108699\n",
      "Loss at step 4420: 0.0953892\n",
      "Loss at step 4430: 0.107695\n",
      "Loss at step 4440: 0.106839\n",
      "Loss at step 4450: 0.112892\n",
      "Loss at step 4460: 0.0981719\n",
      "Loss at step 4470: 0.107183\n",
      "Loss at step 4480: 0.0990926\n",
      "Loss at step 4490: 0.104929\n",
      "Loss at step 4500: 0.106666\n",
      "Loss at step 4510: 0.105028\n",
      "Loss at step 4520: 0.101088\n",
      "Loss at step 4530: 0.101756\n",
      "Loss at step 4540: 0.0989267\n",
      "Loss at step 4550: 0.108146\n",
      "Loss at step 4560: 0.116098\n",
      "Loss at step 4570: 0.110234\n",
      "Loss at step 4580: 0.105462\n",
      "Loss at step 4590: 0.105831\n",
      "Loss at step 4600: 0.103423\n",
      "Loss at step 4610: 0.107624\n",
      "Loss at step 4620: 0.101681\n",
      "Loss at step 4630: 0.105066\n",
      "Loss at step 4640: 0.105156\n",
      "Loss at step 4650: 0.102716\n",
      "Loss at step 4660: 0.110157\n",
      "Loss at step 4670: 0.105932\n",
      "Loss at step 4680: 0.101694\n",
      "Loss at step 4690: 0.108444\n",
      "Loss at step 4700: 0.108931\n",
      "Loss at step 4710: 0.105994\n",
      "Loss at step 4720: 0.0993012\n",
      "Loss at step 4730: 0.107302\n",
      "Loss at step 4740: 0.100346\n",
      "Loss at step 4750: 0.102086\n",
      "Loss at step 4760: 0.10649\n",
      "Loss at step 4770: 0.104475\n",
      "Loss at step 4780: 0.102459\n",
      "Loss at step 4790: 0.103836\n",
      "Loss at step 4800: 0.100096\n",
      "Loss at step 4810: 0.102317\n",
      "Loss at step 4820: 0.104761\n",
      "Loss at step 4830: 0.104979\n",
      "Loss at step 4840: 0.10295\n",
      "Loss at step 4850: 0.114436\n",
      "Loss at step 4860: 0.0998912\n",
      "Loss at step 4870: 0.106253\n",
      "Loss at step 4880: 0.101478\n",
      "Loss at step 4890: 0.103137\n",
      "Loss at step 4900: 0.11504\n",
      "Loss at step 4910: 0.106647\n",
      "Loss at step 4920: 0.104742\n",
      "Loss at step 4930: 0.103525\n",
      "Loss at step 4940: 0.102882\n",
      "Loss at step 4950: 0.104793\n",
      "Loss at step 4960: 0.0984429\n",
      "Loss at step 4970: 0.102466\n",
      "Loss at step 4980: 0.113562\n",
      "Loss at step 4990: 0.108441\n"
     ]
    }
   ],
   "source": [
    "reg=5e-4\n",
    "clf.train(Xtrain,ytrain,n_iters=5000,batch_size=512,learning_rate=7e-4,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10707321, 0.99630952]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for training data\n",
    "clf.score(Xtrain,ytrain,reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15529153, 0.97702378]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for validation data\n",
    "clf.score(Xval,yval,reg=reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss function and accuracy plots visualized using tensorboard with logdir=MLP_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
