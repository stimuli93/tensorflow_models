import numpy as np
import tensorflow as tf
import os


class CharRNN(object):
    def __init__(self, max_seq_length, vocab_size, hidden_size=32, embedding_size=16, ckpt_dir="./ckpt_dir",
                 summary_dir="/tmp/rnn_logs"):
        """
        2-layer LSTM for character modelling
        :param max_seq_length: max length of sequence trained at once
        :param vocab_size: number of unique characters in dataset
        :param hidden_size: size of layers of lstm
        :param embedding_size: size of embedding vector per character
        :param ckpt_dir: directory in which model checkpoints to be stored
        :param summary_dir: directory used as logdir for tensoboard visualization
        """
        num_layers = 2
        self.max_seq_length = max_seq_length
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.ckpt_dir = ckpt_dir
        self.summary_dir = summary_dir

        # Initializing model parameters
        self.weights = self.initialize_weights(vocab_size, hidden_size, embedding_size)
        self.x = tf.placeholder(tf.int32, shape=[1, max_seq_length], name="X")
        self.y = tf.placeholder(tf.int32, shape=[1, max_seq_length], name="y")

        token_embeddings = tf.nn.embedding_lookup(self.weights['W_vocab'], self.x)

        cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, state_is_tuple=True)
        self.cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)

        self.lstm1_c = tf.placeholder(tf.float32, shape=[1, self.hidden_size], name="Lstm1_c")
        self.lstm1_h = tf.placeholder(tf.float32, shape=[1, self.hidden_size], name="Lstm1_h")
        self.lstm2_c = tf.placeholder(tf.float32, shape=[1, self.hidden_size], name="Lstm2_c")
        self.lstm2_h = tf.placeholder(tf.float32, shape=[1, self.hidden_size], name="Lstm2_h")

        state1 = tf.nn.rnn_cell.LSTMStateTuple(self.lstm1_c, self.lstm1_h)
        state2 = tf.nn.rnn_cell.LSTMStateTuple(self.lstm2_c, self.lstm2_h)
        state = (state1, state2)

        outputs = []
        for i in range(max_seq_length):
            if i > 0:
                tf.get_variable_scope().reuse_variables()
            output, state = self.cell(token_embeddings[:, i, :], state)
            outputs.append(output)

        final_state = state
        self.final_lstm1_c = final_state[0].c
        self.final_lstm1_h = final_state[0].h
        self.final_lstm2_c = final_state[1].c
        self.final_lstm2_h = final_state[1].h

        outputs = tf.reshape(tf.concat(1, outputs), [-1, hidden_size])
        logits = tf.matmul(outputs, self.weights['W1']) + self.weights['b1']
        loss = tf.nn.seq2seq.sequence_loss_by_example(
            [logits],
            [tf.reshape(self.y, [-1])],
            [tf.ones([1 * max_seq_length], dtype=tf.float32)])
        self.cost = tf.reduce_sum(loss)
        self.sess = tf.InteractiveSession()

        cost_summ = tf.scalar_summary("loss ", self.cost)

        # Merge all the summaries and write them out to summary_dir
        self.merged = tf.merge_all_summaries()
        self.writer = tf.train.SummaryWriter(self.summary_dir, self.sess.graph)

        if not os.path.exists(self.ckpt_dir):
            os.makedirs(self.ckpt_dir)

        self.global_step = tf.Variable(0, name='global_step', trainable=False)

        # Call this after declaring all tf.Variables.
        self.saver = tf.train.Saver()

        self.sess.run(tf.initialize_all_variables())

    def initialize_weights(self, vocab_size, hidden_size, embedding_size):
        all_weights = dict()
        all_weights['W_vocab'] = tf.Variable(tf.truncated_normal([vocab_size, embedding_size], stddev=0.1))
        all_weights['W1'] = tf.Variable(tf.truncated_normal([hidden_size, vocab_size]))
        all_weights['b1'] = tf.Variable(tf.constant(0.1, shape=[vocab_size]))
        return all_weights

    def sample(self, start, char_count):
        """
        Generates text which is char_count long using model's learnt parameters
        :param start: a list of starting character ids
        :param char_count: the number of characters in the sample text
        :return: list of character ids generated by model
        """
        state = self.cell.zero_state(batch_size=1, dtype=tf.float32)
        output = state
        token_embeddings = tf.nn.embedding_lookup(self.weights['W_vocab'], start)
        generated_text = []
        start_len = len(start)
        for i in range(start_len):
            if i > 0:
                tf.get_variable_scope().reuse_variables()
            char_embedding = tf.reshape(token_embeddings[i, :], shape=[-1, self.embedding_size])
            output, state = self.cell(char_embedding, state)
            generated_text.append(start[i])

        for i in range(char_count-start_len):
            tf.get_variable_scope().reuse_variables()
            probabilities = tf.matmul(output, self.weights['W1']) + self.weights['b1']

            pred_id = tf.multinomial(probabilities, num_samples=1)
            pred_id = self.sess.run(tf.reshape(pred_id, shape=[1]))
            char_embedding = tf.reshape(tf.nn.embedding_lookup(self.weights['W_vocab'], pred_id),
                                        shape=[-1, self.embedding_size])
            output, state = self.cell(char_embedding, state)
            generated_text.append(pred_id[0])
        return generated_text

    def train(self, X, learning_rate=1e-3, nb_epochs=1):
        """
        train method expects X to be the complete text to be trained upon with the specified
        learning_rate for nb_epochs.
        """
        train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.cost)

        # initializing only un-initialized variables to prevent trained variables assigned again with random weights
        uninitialized_vars = []
        for var in tf.all_variables():
            try:
                self.sess.run(var)
            except tf.errors.FailedPreconditionError:
                uninitialized_vars.append(var)

        init_new_vars_op = tf.initialize_variables(uninitialized_vars)
        self.sess.run(init_new_vars_op)

        # restore model
        ckpt = tf.train.get_checkpoint_state(self.ckpt_dir)
        if ckpt and ckpt.model_checkpoint_path:
            print(ckpt.model_checkpoint_path)
            self.saver.restore(self.sess, ckpt.model_checkpoint_path)  # restore all variables

        for ep in range(nb_epochs):
            start = self.global_step.eval()  # get last global_step
            x_len = len(X)
            hidden_size = self.hidden_size
            max_seq_length = self.max_seq_length

            iterations = (x_len-1)//max_seq_length
            print 'Performing %d iterations in 1 epoch' % iterations

            # Assigning lstm parameters to 0 only at start of the epoch
            lstm1_c = np.zeros([1, hidden_size])
            lstm1_h = np.zeros([1, hidden_size])
            lstm2_c = np.zeros([1, hidden_size])
            lstm2_h = np.zeros([1, hidden_size])

            for i in xrange(0, iterations):
                iter_start = i*max_seq_length
                trX = np.array(X[iter_start: iter_start+max_seq_length]).reshape([1, max_seq_length])
                trY = np.array(X[iter_start+1: iter_start+max_seq_length+1]).reshape([1, max_seq_length])
                feed_dict = {self.x: trX, self.y: trY,
                             self.lstm1_c: lstm1_c, self.lstm1_h: lstm1_h, self.lstm2_c: lstm2_c, self.lstm2_h: lstm2_h}
                lstm1_c, lstm1_h, lstm2_c, lstm2_h, _ = self.sess.run([self.final_lstm1_c, self.final_lstm1_h,
                                                                       self.final_lstm2_c, self.final_lstm2_h,
                                                                       train_step], feed_dict=feed_dict)

                if i % 10 == 0:
                    result = self.sess.run([self.merged, self.cost],
                                           feed_dict=feed_dict)
                    summary_str = result[0]
                    loss = result[1]
                    self.writer.add_summary(summary_str, i + start)
                    print("Loss at step %s: %s" % (i + start, loss))
                    if i % 500 == 0:
                        self.global_step.assign(i + start).eval()  # set and update(eval) global_step
                        self.saver.save(self.sess, self.ckpt_dir + "/model.ckpt", global_step=self.global_step)

            self.global_step.assign(iterations + start).eval()  # set and update(eval) global_step
            self.saver.save(self.sess, self.ckpt_dir + "/model.ckpt", global_step=self.global_step)

