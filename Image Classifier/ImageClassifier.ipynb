{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification using model built in Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MNIST data to test the functionality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = train.drop('label',axis=1).values\n",
    "trX = trX.reshape([-1,28,28])\n",
    "trY = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY = pd.get_dummies(trY).values\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input images are of size (28x28) but the model expects input image size to be multiple of 8 so padding zeros at the border to make image size (32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = trX.shape[0]\n",
    "resized_image = []\n",
    "for i in xrange(n_samples):\n",
    "    img = trX[i]\n",
    "    img = np.lib.pad(img, (2,2), 'constant', constant_values=(0))\n",
    "    resized_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX = np.array(resized_image).reshape([-1,32,32,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12eedb610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD9CAYAAACcAsr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+ISkkH8ErT4D3KIvGooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1051c43d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trX[0].reshape(32,32),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX = trX/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain,Xval,ytrain,yval = train_test_split(trX,trY,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ImageClassifier import ConvolutionalClassifier\n",
    "clf = ConvolutionalClassifier(n_classes=10,img_dims=trX[0].shape, ckpt_dir=\"./ckpt_dir\", summary_dir=\"/tmp/convClf_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ckpt_dir/model.ckpt-14000\n",
      "Loss at step 14000: 0.000783717\n",
      "Loss at step 14010: 0.000165834\n",
      "Loss at step 14020: 0.00220895\n",
      "Loss at step 14030: 0.0390278\n",
      "Loss at step 14040: 5.87713e-06\n",
      "Loss at step 14050: 0.0250071\n",
      "Loss at step 14060: 0.000852887\n",
      "Loss at step 14070: 0.0979741\n",
      "Loss at step 14080: 2.23026e-05\n",
      "Loss at step 14090: 0.0197195\n",
      "Loss at step 14100: 0.182181\n",
      "Loss at step 14110: 0.00494025\n",
      "Loss at step 14120: 0.000394644\n",
      "Loss at step 14130: 0.0067536\n",
      "Loss at step 14140: 3.43001e-05\n",
      "Loss at step 14150: 4.30353e-06\n",
      "Loss at step 14160: 0.00189605\n",
      "Loss at step 14170: 0.000122177\n",
      "Loss at step 14180: 0.0010358\n",
      "Loss at step 14190: 0.0979133\n",
      "Loss at step 14200: 0.000769445\n",
      "Loss at step 14210: 0.00131037\n",
      "Loss at step 14220: 0.0196502\n",
      "Loss at step 14230: 0.00787526\n",
      "Loss at step 14240: 2.1578e-05\n",
      "Loss at step 14250: 0.0634497\n",
      "Loss at step 14260: 0.144282\n",
      "Loss at step 14270: 0.0138611\n",
      "Loss at step 14280: 0.00643823\n",
      "Loss at step 14290: 0.000328499\n",
      "Loss at step 14300: 0.0431006\n",
      "Loss at step 14310: 0.00167328\n",
      "Loss at step 14320: 0.000105641\n",
      "Loss at step 14330: 0.00623026\n",
      "Loss at step 14340: 0.558944\n",
      "Loss at step 14350: 0.00072386\n",
      "Loss at step 14360: 0.0856068\n",
      "Loss at step 14370: 0.000287783\n",
      "Loss at step 14380: 0.00570072\n",
      "Loss at step 14390: 0.00407685\n",
      "Loss at step 14400: 0.120547\n",
      "Loss at step 14410: 0.0271661\n",
      "Loss at step 14420: 0.0460163\n",
      "Loss at step 14430: 9.52929e-05\n",
      "Loss at step 14440: 0.00846204\n",
      "Loss at step 14450: 0.095226\n",
      "Loss at step 14460: 0.00113382\n",
      "Loss at step 14470: 0.00631855\n",
      "Loss at step 14480: 0.0158257\n",
      "Loss at step 14490: 0.000350767\n",
      "Loss at step 14500: 0.00327232\n",
      "Loss at step 14510: 0.000194426\n",
      "Loss at step 14520: 0.00964467\n",
      "Loss at step 14530: 0.00921963\n",
      "Loss at step 14540: 0.00115133\n",
      "Loss at step 14550: 0.0342132\n",
      "Loss at step 14560: 0.0177132\n",
      "Loss at step 14570: 0.187256\n",
      "Loss at step 14580: 0.021173\n",
      "Loss at step 14590: 0.00190529\n",
      "Loss at step 14600: 0.128614\n",
      "Loss at step 14610: 0.00253768\n",
      "Loss at step 14620: 0.00814984\n",
      "Loss at step 14630: 0.071523\n",
      "Loss at step 14640: 0.00255904\n",
      "Loss at step 14650: 0.00376544\n",
      "Loss at step 14660: 0.24393\n",
      "Loss at step 14670: 0.116487\n",
      "Loss at step 14680: 0.151395\n",
      "Loss at step 14690: 0.226239\n",
      "Loss at step 14700: 0.000508768\n",
      "Loss at step 14710: 0.00116249\n",
      "Loss at step 14720: 0.1514\n",
      "Loss at step 14730: 0.00313928\n",
      "Loss at step 14740: 0.0687947\n",
      "Loss at step 14750: 0.000617886\n",
      "Loss at step 14760: 0.0160926\n",
      "Loss at step 14770: 0.000421892\n",
      "Loss at step 14780: 0.00401169\n",
      "Loss at step 14790: 0.007904\n",
      "Loss at step 14800: 0.0124273\n",
      "Loss at step 14810: 0.00167154\n",
      "Loss at step 14820: 0.00237955\n",
      "Loss at step 14830: 0.3489\n",
      "Loss at step 14840: 0.0067033\n",
      "Loss at step 14850: 0.00517169\n",
      "Loss at step 14860: 0.0990825\n",
      "Loss at step 14870: 0.0958277\n",
      "Loss at step 14880: 0.0201381\n",
      "Loss at step 14890: 0.0738771\n",
      "Loss at step 14900: 0.12807\n",
      "Loss at step 14910: 0.000399977\n",
      "Loss at step 14920: 0.00312894\n",
      "Loss at step 14930: 0.00164678\n",
      "Loss at step 14940: 0.000405444\n",
      "Loss at step 14950: 0.00357452\n",
      "Loss at step 14960: 0.000365871\n",
      "Loss at step 14970: 0.00615408\n",
      "Loss at step 14980: 0.147325\n",
      "Loss at step 14990: 0.00585396\n",
      "Loss at step 15000: 0.0356052\n",
      "Loss at step 15010: 0.00149805\n",
      "Loss at step 15020: 0.00318683\n",
      "Loss at step 15030: 0.115783\n",
      "Loss at step 15040: 0.102675\n",
      "Loss at step 15050: 0.0035494\n",
      "Loss at step 15060: 0.0065948\n",
      "Loss at step 15070: 0.00237508\n",
      "Loss at step 15080: 0.0213322\n",
      "Loss at step 15090: 0.0351862\n",
      "Loss at step 15100: 0.185507\n",
      "Loss at step 15110: 0.0168792\n",
      "Loss at step 15120: 0.0353375\n",
      "Loss at step 15130: 0.00020934\n",
      "Loss at step 15140: 0.0049643\n",
      "Loss at step 15150: 0.000558779\n",
      "Loss at step 15160: 0.0661621\n",
      "Loss at step 15170: 8.29783e-05\n",
      "Loss at step 15180: 0.0245415\n",
      "Loss at step 15190: 0.00172842\n",
      "Loss at step 15200: 0.0214871\n",
      "Loss at step 15210: 0.058355\n",
      "Loss at step 15220: 0.202663\n",
      "Loss at step 15230: 0.000459313\n",
      "Loss at step 15240: 0.00018057\n",
      "Loss at step 15250: 0.0115204\n",
      "Loss at step 15260: 0.173767\n",
      "Loss at step 15270: 0.128116\n",
      "Loss at step 15280: 0.0793903\n",
      "Loss at step 15290: 0.0174481\n",
      "Loss at step 15300: 0.000388441\n",
      "Loss at step 15310: 0.0164448\n",
      "Loss at step 15320: 0.00194667\n",
      "Loss at step 15330: 0.00082403\n",
      "Loss at step 15340: 0.0670771\n",
      "Loss at step 15350: 0.113453\n",
      "Loss at step 15360: 0.0316639\n",
      "Loss at step 15370: 0.352642\n",
      "Loss at step 15380: 0.0866746\n",
      "Loss at step 15390: 0.0379405\n",
      "Loss at step 15400: 0.0211781\n",
      "Loss at step 15410: 0.010722\n",
      "Loss at step 15420: 0.187257\n",
      "Loss at step 15430: 9.19606e-05\n",
      "Loss at step 15440: 0.142982\n",
      "Loss at step 15450: 0.0121916\n",
      "Loss at step 15460: 0.0245371\n",
      "Loss at step 15470: 0.0351299\n",
      "Loss at step 15480: 0.00535062\n",
      "Loss at step 15490: 0.00424194\n",
      "Loss at step 15500: 0.00371692\n",
      "Loss at step 15510: 5.15759e-05\n",
      "Loss at step 15520: 0.000561763\n",
      "Loss at step 15530: 0.000208675\n",
      "Loss at step 15540: 0.00185803\n",
      "Loss at step 15550: 0.109757\n",
      "Loss at step 15560: 0.000183693\n",
      "Loss at step 15570: 0.0161225\n",
      "Loss at step 15580: 0.0042427\n",
      "Loss at step 15590: 0.000765514\n",
      "Loss at step 15600: 0.00600934\n",
      "Loss at step 15610: 0.000560649\n",
      "Loss at step 15620: 0.000563743\n",
      "Loss at step 15630: 0.00873862\n",
      "Loss at step 15640: 0.0216459\n",
      "Loss at step 15650: 0.0576587\n",
      "Loss at step 15660: 0.000495282\n",
      "Loss at step 15670: 0.203654\n",
      "Loss at step 15680: 0.00294328\n",
      "Loss at step 15690: 0.00582021\n",
      "Loss at step 15700: 0.0260297\n",
      "Loss at step 15710: 0.00750617\n",
      "Loss at step 15720: 0.0674592\n",
      "Loss at step 15730: 0.189473\n",
      "Loss at step 15740: 0.061644\n",
      "Loss at step 15750: 0.0668508\n",
      "Loss at step 15760: 0.112189\n",
      "Loss at step 15770: 0.174702\n",
      "Loss at step 15780: 0.000312132\n",
      "Loss at step 15790: 0.0937046\n",
      "Loss at step 15800: 0.0988182\n",
      "Loss at step 15810: 0.182409\n",
      "Loss at step 15820: 0.00424521\n",
      "Loss at step 15830: 0.105211\n",
      "Loss at step 15840: 0.0196224\n",
      "Loss at step 15850: 0.000942068\n",
      "Loss at step 15860: 0.000270355\n",
      "Loss at step 15870: 0.171286\n",
      "Loss at step 15880: 0.146799\n",
      "Loss at step 15890: 0.00310988\n",
      "Loss at step 15900: 0.00949063\n",
      "Loss at step 15910: 0.129461\n",
      "Loss at step 15920: 0.276939\n",
      "Loss at step 15930: 0.00272079\n",
      "Loss at step 15940: 0.006801\n",
      "Loss at step 15950: 0.0454886\n",
      "Loss at step 15960: 0.0550723\n",
      "Loss at step 15970: 0.0693621\n",
      "Loss at step 15980: 0.000526474\n",
      "Loss at step 15990: 0.0213202\n",
      "Loss at step 16000: 0.249934\n",
      "Loss at step 16010: 0.000290943\n",
      "Loss at step 16020: 0.00166656\n",
      "Loss at step 16030: 0.0391109\n",
      "Loss at step 16040: 0.00713718\n",
      "Loss at step 16050: 0.0165885\n",
      "Loss at step 16060: 6.88269e-05\n",
      "Loss at step 16070: 0.00455342\n",
      "Loss at step 16080: 0.00768788\n",
      "Loss at step 16090: 0.00195675\n",
      "Loss at step 16100: 0.00588633\n",
      "Loss at step 16110: 0.00344955\n",
      "Loss at step 16120: 0.00863345\n",
      "Loss at step 16130: 0.0104615\n",
      "Loss at step 16140: 0.0451014\n",
      "Loss at step 16150: 0.00314272\n",
      "Loss at step 16160: 0.0339401\n",
      "Loss at step 16170: 0.013534\n",
      "Loss at step 16180: 0.000668361\n",
      "Loss at step 16190: 0.0010665\n",
      "Loss at step 16200: 0.0155187\n",
      "Loss at step 16210: 0.0154311\n",
      "Loss at step 16220: 0.0223041\n",
      "Loss at step 16230: 4.39892e-06\n",
      "Loss at step 16240: 0.00868848\n",
      "Loss at step 16250: 0.0870116\n",
      "Loss at step 16260: 0.000614163\n",
      "Loss at step 16270: 0.000509416\n",
      "Loss at step 16280: 0.0471274\n",
      "Loss at step 16290: 0.00545614\n",
      "Loss at step 16300: 0.000156202\n",
      "Loss at step 16310: 0.00102189\n",
      "Loss at step 16320: 0.0336724\n",
      "Loss at step 16330: 0.00228832\n",
      "Loss at step 16340: 0.000359704\n",
      "Loss at step 16350: 2.80946e-05\n",
      "Loss at step 16360: 0.0463389\n",
      "Loss at step 16370: 0.000341598\n",
      "Loss at step 16380: 0.0121801\n",
      "Loss at step 16390: 0.0367204\n",
      "Loss at step 16400: 0.132235\n",
      "Loss at step 16410: 0.285471\n",
      "Loss at step 16420: 0.00237584\n",
      "Loss at step 16430: 0.00954628\n",
      "Loss at step 16440: 0.00023005\n",
      "Loss at step 16450: 0.0603005\n",
      "Loss at step 16460: 0.27538\n",
      "Loss at step 16470: 0.000175856\n",
      "Loss at step 16480: 0.018134\n",
      "Loss at step 16490: 0.0153229\n",
      "Loss at step 16500: 0.00106366\n",
      "Loss at step 16510: 0.000132314\n",
      "Loss at step 16520: 0.153063\n",
      "Loss at step 16530: 0.00116753\n",
      "Loss at step 16540: 0.00103755\n",
      "Loss at step 16550: 0.0391466\n",
      "Loss at step 16560: 0.247439\n",
      "Loss at step 16570: 0.15774\n",
      "Loss at step 16580: 0.000742669\n",
      "Loss at step 16590: 0.00243395\n",
      "Loss at step 16600: 0.000741633\n",
      "Loss at step 16610: 0.00152197\n",
      "Loss at step 16620: 2.59353e-05\n",
      "Loss at step 16630: 0.0226919\n",
      "Loss at step 16640: 0.0399035\n",
      "Loss at step 16650: 8.42496e-05\n",
      "Loss at step 16660: 0.0959787\n",
      "Loss at step 16670: 0.00387965\n",
      "Loss at step 16680: 0.00114159\n",
      "Loss at step 16690: 0.00115055\n",
      "Loss at step 16700: 0.0300958\n",
      "Loss at step 16710: 0.00684654\n",
      "Loss at step 16720: 2.80739e-06\n",
      "Loss at step 16730: 0.0471995\n",
      "Loss at step 16740: 0.202841\n",
      "Loss at step 16750: 0.0541388\n",
      "Loss at step 16760: 0.000411888\n",
      "Loss at step 16770: 0.000239393\n",
      "Loss at step 16780: 0.00508104\n",
      "Loss at step 16790: 0.106217\n",
      "Loss at step 16800: 0.0219982\n",
      "Loss at step 16810: 0.0083792\n",
      "Loss at step 16820: 0.001062\n",
      "Loss at step 16830: 0.0174162\n",
      "Loss at step 16840: 0.0638823\n",
      "Loss at step 16850: 0.0014088\n",
      "Loss at step 16860: 0.0016179\n",
      "Loss at step 16870: 1.09914e-05\n",
      "Loss at step 16880: 0.0202376\n",
      "Loss at step 16890: 0.000221678\n",
      "Loss at step 16900: 0.0218636\n",
      "Loss at step 16910: 0.0166691\n",
      "Loss at step 16920: 0.0591788\n",
      "Loss at step 16930: 0.0127928\n",
      "Loss at step 16940: 0.156612\n",
      "Loss at step 16950: 0.0515255\n",
      "Loss at step 16960: 0.0210945\n",
      "Loss at step 16970: 0.0865732\n",
      "Loss at step 16980: 0.0012924\n",
      "Loss at step 16990: 0.00143346\n",
      "Loss at step 17000: 0.00228282\n",
      "Loss at step 17010: 0.05222\n",
      "Loss at step 17020: 0.072825\n",
      "Loss at step 17030: 0.00267735\n",
      "Loss at step 17040: 0.000857741\n",
      "Loss at step 17050: 0.00427279\n",
      "Loss at step 17060: 0.00119427\n",
      "Loss at step 17070: 0.162913\n",
      "Loss at step 17080: 0.0181863\n",
      "Loss at step 17090: 0.000247179\n",
      "Loss at step 17100: 0.00116748\n",
      "Loss at step 17110: 0.0968329\n",
      "Loss at step 17120: 0.0257095\n",
      "Loss at step 17130: 0.00751323\n",
      "Loss at step 17140: 0.0195474\n",
      "Loss at step 17150: 0.00146856\n",
      "Loss at step 17160: 0.0553118\n",
      "Loss at step 17170: 0.00171292\n",
      "Loss at step 17180: 0.000396865\n",
      "Loss at step 17190: 0.133334\n",
      "Loss at step 17200: 0.00034465\n",
      "Loss at step 17210: 0.0850754\n",
      "Loss at step 17220: 0.0238707\n",
      "Loss at step 17230: 0.199503\n",
      "Loss at step 17240: 0.0818836\n",
      "Loss at step 17250: 0.143866\n",
      "Loss at step 17260: 0.00499378\n",
      "Loss at step 17270: 0.00117781\n",
      "Loss at step 17280: 0.000245146\n",
      "Loss at step 17290: 0.00301527\n",
      "Loss at step 17300: 0.00142036\n",
      "Loss at step 17310: 0.00124487\n",
      "Loss at step 17320: 0.00379877\n",
      "Loss at step 17330: 0.00515499\n",
      "Loss at step 17340: 0.0382551\n",
      "Loss at step 17350: 0.0331845\n",
      "Loss at step 17360: 0.0161922\n",
      "Loss at step 17370: 0.000440765\n",
      "Loss at step 17380: 0.12563\n",
      "Loss at step 17390: 0.0757728\n",
      "Loss at step 17400: 0.236067\n",
      "Loss at step 17410: 0.0126954\n",
      "Loss at step 17420: 0.00528578\n",
      "Loss at step 17430: 0.0929811\n",
      "Loss at step 17440: 0.00902949\n",
      "Loss at step 17450: 0.00104727\n",
      "Loss at step 17460: 0.00221822\n",
      "Loss at step 17470: 0.00058517\n",
      "Loss at step 17480: 0.00367846\n",
      "Loss at step 17490: 0.00447151\n",
      "Loss at step 17500: 9.05542e-05\n",
      "Loss at step 17510: 0.00113131\n",
      "Loss at step 17520: 0.0184517\n",
      "Loss at step 17530: 0.00392938\n",
      "Loss at step 17540: 0.00284351\n",
      "Loss at step 17550: 0.000920219\n",
      "Loss at step 17560: 5.11735e-05\n",
      "Loss at step 17570: 0.367543\n",
      "Loss at step 17580: 0.00736806\n",
      "Loss at step 17590: 0.00170464\n",
      "Loss at step 17600: 0.00519683\n",
      "Loss at step 17610: 0.00600995\n",
      "Loss at step 17620: 0.000119288\n",
      "Loss at step 17630: 0.00619468\n",
      "Loss at step 17640: 0.00246075\n",
      "Loss at step 17650: 0.00108195\n",
      "Loss at step 17660: 0.0205022\n",
      "Loss at step 17670: 0.000428602\n",
      "Loss at step 17680: 0.0109695\n",
      "Loss at step 17690: 5.02368e-05\n",
      "Loss at step 17700: 0.0191174\n",
      "Loss at step 17710: 0.00353238\n",
      "Loss at step 17720: 0.0191021\n",
      "Loss at step 17730: 0.010725\n",
      "Loss at step 17740: 0.00554386\n",
      "Loss at step 17750: 0.0032129\n",
      "Loss at step 17760: 0.122788\n",
      "Loss at step 17770: 0.00229646\n",
      "Loss at step 17780: 0.0112684\n",
      "Loss at step 17790: 0.0128918\n",
      "Loss at step 17800: 0.00160398\n",
      "Loss at step 17810: 0.00180251\n",
      "Loss at step 17820: 0.000172528\n",
      "Loss at step 17830: 1.77112e-05\n",
      "Loss at step 17840: 0.0207162\n",
      "Loss at step 17850: 0.0342671\n",
      "Loss at step 17860: 0.715948\n",
      "Loss at step 17870: 0.000987332\n",
      "Loss at step 17880: 0.155004\n",
      "Loss at step 17890: 0.00602571\n",
      "Loss at step 17900: 0.0209321\n",
      "Loss at step 17910: 0.000529211\n",
      "Loss at step 17920: 0.0037488\n",
      "Loss at step 17930: 0.00913913\n",
      "Loss at step 17940: 0.00173156\n",
      "Loss at step 17950: 0.446228\n",
      "Loss at step 17960: 0.00336321\n",
      "Loss at step 17970: 0.061016\n",
      "Loss at step 17980: 0.0182641\n",
      "Loss at step 17990: 0.00180365\n",
      "Loss at step 18000: 0.109987\n",
      "Loss at step 18010: 0.00767\n",
      "Loss at step 18020: 0.208191\n",
      "Loss at step 18030: 0.000456474\n",
      "Loss at step 18040: 0.0182967\n",
      "Loss at step 18050: 0.0470869\n",
      "Loss at step 18060: 0.00610119\n",
      "Loss at step 18070: 0.00254699\n",
      "Loss at step 18080: 0.00330144\n",
      "Loss at step 18090: 0.00614566\n",
      "Loss at step 18100: 0.00592727\n",
      "Loss at step 18110: 0.0094635\n",
      "Loss at step 18120: 8.25319e-05\n",
      "Loss at step 18130: 0.00617506\n",
      "Loss at step 18140: 0.0207538\n",
      "Loss at step 18150: 0.000981221\n",
      "Loss at step 18160: 0.00240714\n",
      "Loss at step 18170: 5.27176e-05\n",
      "Loss at step 18180: 0.00928409\n",
      "Loss at step 18190: 0.00245004\n",
      "Loss at step 18200: 0.0473801\n",
      "Loss at step 18210: 0.0240845\n",
      "Loss at step 18220: 0.00108248\n",
      "Loss at step 18230: 0.000369886\n",
      "Loss at step 18240: 0.059702\n",
      "Loss at step 18250: 0.0680619\n",
      "Loss at step 18260: 0.155644\n",
      "Loss at step 18270: 0.00308237\n",
      "Loss at step 18280: 0.0012983\n",
      "Loss at step 18290: 0.000299543\n",
      "Loss at step 18300: 0.00191163\n",
      "Loss at step 18310: 0.14018\n",
      "Loss at step 18320: 0.000263686\n",
      "Loss at step 18330: 0.0270476\n",
      "Loss at step 18340: 5.68425e-05\n",
      "Loss at step 18350: 6.29681e-05\n",
      "Loss at step 18360: 0.00116631\n",
      "Loss at step 18370: 0.00670375\n",
      "Loss at step 18380: 0.00929098\n",
      "Loss at step 18390: 0.000947394\n",
      "Loss at step 18400: 0.00198531\n",
      "Loss at step 18410: 0.00656407\n",
      "Loss at step 18420: 0.0152929\n",
      "Loss at step 18430: 0.000744155\n",
      "Loss at step 18440: 0.00104541\n",
      "Loss at step 18450: 0.218484\n",
      "Loss at step 18460: 0.0040909\n",
      "Loss at step 18470: 0.00141755\n",
      "Loss at step 18480: 0.00542519\n",
      "Loss at step 18490: 0.269814\n",
      "Loss at step 18500: 0.0167682\n",
      "Loss at step 18510: 0.00666939\n",
      "Loss at step 18520: 0.0432453\n",
      "Loss at step 18530: 0.0867949\n",
      "Loss at step 18540: 0.0915465\n",
      "Loss at step 18550: 0.015464\n",
      "Loss at step 18560: 0.00506796\n",
      "Loss at step 18570: 0.119731\n",
      "Loss at step 18580: 0.0674711\n",
      "Loss at step 18590: 0.217522\n",
      "Loss at step 18600: 0.201525\n",
      "Loss at step 18610: 0.158845\n",
      "Loss at step 18620: 0.00250443\n",
      "Loss at step 18630: 1.56169e-05\n",
      "Loss at step 18640: 0.042546\n",
      "Loss at step 18650: 0.161627\n",
      "Loss at step 18660: 0.0382192\n",
      "Loss at step 18670: 0.261231\n",
      "Loss at step 18680: 0.000143516\n",
      "Loss at step 18690: 0.119526\n",
      "Loss at step 18700: 0.00026887\n",
      "Loss at step 18710: 0.0330591\n",
      "Loss at step 18720: 0.00775035\n",
      "Loss at step 18730: 0.0258365\n",
      "Loss at step 18740: 0.000952293\n",
      "Loss at step 18750: 0.0297953\n",
      "Loss at step 18760: 5.48904e-05\n",
      "Loss at step 18770: 0.00877893\n",
      "Loss at step 18780: 0.000370519\n",
      "Loss at step 18790: 0.00104585\n",
      "Loss at step 18800: 0.190728\n",
      "Loss at step 18810: 0.068792\n",
      "Loss at step 18820: 0.00787081\n",
      "Loss at step 18830: 0.194836\n",
      "Loss at step 18840: 0.012135\n",
      "Loss at step 18850: 0.000240575\n",
      "Loss at step 18860: 0.0876055\n",
      "Loss at step 18870: 3.09528e-05\n",
      "Loss at step 18880: 0.00333046\n",
      "Loss at step 18890: 0.136782\n",
      "Loss at step 18900: 0.0125001\n",
      "Loss at step 18910: 0.00831986\n",
      "Loss at step 18920: 0.00638234\n",
      "Loss at step 18930: 0.000467051\n",
      "Loss at step 18940: 0.00131031\n",
      "Loss at step 18950: 0.451402\n",
      "Loss at step 18960: 0.000583532\n",
      "Loss at step 18970: 0.0956713\n",
      "Loss at step 18980: 0.0680058\n",
      "Loss at step 18990: 0.0284379\n",
      "Loss at step 19000: 0.145713\n",
      "Loss at step 19010: 0.0809757\n",
      "Loss at step 19020: 0.034211\n",
      "Loss at step 19030: 0.069578\n",
      "Loss at step 19040: 0.00789412\n",
      "Loss at step 19050: 0.000671679\n",
      "Loss at step 19060: 0.0262425\n",
      "Loss at step 19070: 0.123037\n",
      "Loss at step 19080: 0.0086465\n",
      "Loss at step 19090: 0.000939348\n",
      "Loss at step 19100: 0.000574403\n",
      "Loss at step 19110: 0.148701\n",
      "Loss at step 19120: 0.00468595\n",
      "Loss at step 19130: 0.000138499\n",
      "Loss at step 19140: 0.111267\n",
      "Loss at step 19150: 0.0119281\n",
      "Loss at step 19160: 0.193301\n",
      "Loss at step 19170: 0.171131\n",
      "Loss at step 19180: 0.0213025\n",
      "Loss at step 19190: 0.00102772\n",
      "Loss at step 19200: 0.000466822\n",
      "Loss at step 19210: 0.0891564\n",
      "Loss at step 19220: 0.002052\n",
      "Loss at step 19230: 0.00191841\n",
      "Loss at step 19240: 0.011221\n",
      "Loss at step 19250: 2.02368e-05\n",
      "Loss at step 19260: 0.056344\n",
      "Loss at step 19270: 0.000343818\n",
      "Loss at step 19280: 0.0511034\n",
      "Loss at step 19290: 0.0654427\n",
      "Loss at step 19300: 0.28452\n",
      "Loss at step 19310: 0.111101\n",
      "Loss at step 19320: 0.0240197\n",
      "Loss at step 19330: 2.59907e-05\n",
      "Loss at step 19340: 0.00694034\n",
      "Loss at step 19350: 0.0195527\n",
      "Loss at step 19360: 0.00150464\n",
      "Loss at step 19370: 0.123559\n",
      "Loss at step 19380: 0.00022866\n",
      "Loss at step 19390: 0.167316\n",
      "Loss at step 19400: 0.0614839\n",
      "Loss at step 19410: 0.00358003\n",
      "Loss at step 19420: 0.0038576\n",
      "Loss at step 19430: 0.0795961\n",
      "Loss at step 19440: 0.00426737\n",
      "Loss at step 19450: 0.0153729\n",
      "Loss at step 19460: 0.0323262\n",
      "Loss at step 19470: 0.0457906\n",
      "Loss at step 19480: 0.0138592\n",
      "Loss at step 19490: 0.0033486\n",
      "Loss at step 19500: 3.00358e-05\n",
      "Loss at step 19510: 0.0321522\n",
      "Loss at step 19520: 0.00160924\n",
      "Loss at step 19530: 6.44405e-05\n",
      "Loss at step 19540: 0.0964297\n",
      "Loss at step 19550: 0.000982946\n",
      "Loss at step 19560: 0.00595362\n",
      "Loss at step 19570: 0.0076517\n",
      "Loss at step 19580: 0.00112536\n",
      "Loss at step 19590: 0.0025953\n",
      "Loss at step 19600: 0.000307099\n",
      "Loss at step 19610: 0.147527\n",
      "Loss at step 19620: 0.175611\n",
      "Loss at step 19630: 0.0469707\n",
      "Loss at step 19640: 0.00154621\n",
      "Loss at step 19650: 0.010481\n",
      "Loss at step 19660: 0.00124254\n",
      "Loss at step 19670: 0.013497\n",
      "Loss at step 19680: 0.000469482\n",
      "Loss at step 19690: 0.0200845\n",
      "Loss at step 19700: 0.0409788\n",
      "Loss at step 19710: 0.00479832\n",
      "Loss at step 19720: 0.0754118\n",
      "Loss at step 19730: 0.00664803\n",
      "Loss at step 19740: 0.000147805\n",
      "Loss at step 19750: 0.123889\n",
      "Loss at step 19760: 0.000363369\n",
      "Loss at step 19770: 3.39639e-05\n",
      "Loss at step 19780: 0.0248659\n",
      "Loss at step 19790: 0.118082\n",
      "Loss at step 19800: 0.00031599\n",
      "Loss at step 19810: 0.00288271\n",
      "Loss at step 19820: 0.416946\n",
      "Loss at step 19830: 0.0137346\n",
      "Loss at step 19840: 0.00450185\n",
      "Loss at step 19850: 0.00408241\n",
      "Loss at step 19860: 0.000960326\n",
      "Loss at step 19870: 0.0307766\n",
      "Loss at step 19880: 0.00093071\n",
      "Loss at step 19890: 0.00371116\n",
      "Loss at step 19900: 0.0561214\n",
      "Loss at step 19910: 0.196699\n",
      "Loss at step 19920: 0.0187717\n",
      "Loss at step 19930: 0.204329\n",
      "Loss at step 19940: 0.0043587\n",
      "Loss at step 19950: 0.0317277\n",
      "Loss at step 19960: 0.00211847\n",
      "Loss at step 19970: 0.000294805\n",
      "Loss at step 19980: 0.0199477\n",
      "Loss at step 19990: 5.84588e-05\n"
     ]
    }
   ],
   "source": [
    "clf.train(Xtrain,ytrain,n_iters=6000,batch_size=20,learning_rate=7e-4,keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy and loss for training data\n",
    "clf.score(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy and loss for validation data\n",
    "clf.score(Xval,yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss function and accuracy plots visualized using tensorboard with logdir=convClf_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
