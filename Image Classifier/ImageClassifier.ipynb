{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification using model built in Tensorflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using MNIST data to test the functionality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trX = train.drop('label',axis=1).values\n",
    "trX = trX.reshape([-1,28,28])\n",
    "trY = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trY = pd.get_dummies(trY).values\n",
    "trY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input images are of size (28x28) but the model expects input image size to be multiple of 8 so padding zeros at the border to make image size (32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples = trX.shape[0]\n",
    "resized_image = []\n",
    "for i in xrange(n_samples):\n",
    "    img = trX[i]\n",
    "    img = np.lib.pad(img, (2,2), 'constant', constant_values=(0))\n",
    "    resized_image.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX = np.array(resized_image).reshape([-1,32,32,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12e4e8610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD9CAYAAACcAsr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+ISkkH8ErT4D3KIvGooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1168c3390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trX[0].reshape(32,32),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trX = trX/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain,Xval,ytrain,yval = train_test_split(trX,trY,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ImageClassifier import ConvolutionalClassifier\n",
    "clf = ConvolutionalClassifier(n_classes=10,img_dims=trX[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 2.28727\n",
      "Loss at step 10: 2.35246\n",
      "Loss at step 20: 2.3098\n",
      "Loss at step 30: 2.26058\n",
      "Loss at step 40: 2.29574\n",
      "Loss at step 50: 2.30639\n",
      "Loss at step 60: 2.30955\n",
      "Loss at step 70: 2.29447\n",
      "Loss at step 80: 2.28883\n",
      "Loss at step 90: 2.30933\n",
      "Loss at step 100: 2.28252\n",
      "Loss at step 110: 2.27603\n",
      "Loss at step 120: 2.30688\n",
      "Loss at step 130: 2.27051\n",
      "Loss at step 140: 2.31147\n",
      "Loss at step 150: 2.26909\n",
      "Loss at step 160: 2.26839\n",
      "Loss at step 170: 2.21736\n",
      "Loss at step 180: 2.24162\n",
      "Loss at step 190: 2.25165\n",
      "Loss at step 200: 2.16506\n",
      "Loss at step 210: 2.19543\n",
      "Loss at step 220: 2.09392\n",
      "Loss at step 230: 2.08596\n",
      "Loss at step 240: 1.81258\n",
      "Loss at step 250: 1.76016\n",
      "Loss at step 260: 1.91984\n",
      "Loss at step 270: 1.63146\n",
      "Loss at step 280: 1.38414\n",
      "Loss at step 290: 1.36652\n",
      "Loss at step 300: 1.11573\n",
      "Loss at step 310: 1.75332\n",
      "Loss at step 320: 1.29603\n",
      "Loss at step 330: 1.09903\n",
      "Loss at step 340: 0.929128\n",
      "Loss at step 350: 1.2202\n",
      "Loss at step 360: 0.896245\n",
      "Loss at step 370: 0.817039\n",
      "Loss at step 380: 1.05041\n",
      "Loss at step 390: 0.814649\n",
      "Loss at step 400: 0.964342\n",
      "Loss at step 410: 1.00752\n",
      "Loss at step 420: 0.633476\n",
      "Loss at step 430: 0.761326\n",
      "Loss at step 440: 0.66726\n",
      "Loss at step 450: 0.729003\n",
      "Loss at step 460: 0.757301\n",
      "Loss at step 470: 0.809021\n",
      "Loss at step 480: 0.701368\n",
      "Loss at step 490: 0.543437\n",
      "Loss at step 500: 0.572079\n",
      "Loss at step 510: 0.334406\n",
      "Loss at step 520: 0.551807\n",
      "Loss at step 530: 0.568392\n",
      "Loss at step 540: 0.924879\n",
      "Loss at step 550: 0.42145\n",
      "Loss at step 560: 0.676191\n",
      "Loss at step 570: 0.309595\n",
      "Loss at step 580: 0.347916\n",
      "Loss at step 590: 0.515382\n",
      "Loss at step 600: 0.339962\n",
      "Loss at step 610: 0.398597\n",
      "Loss at step 620: 0.330289\n",
      "Loss at step 630: 0.500557\n",
      "Loss at step 640: 0.344295\n",
      "Loss at step 650: 0.338756\n",
      "Loss at step 660: 0.192216\n",
      "Loss at step 670: 0.228095\n",
      "Loss at step 680: 0.49736\n",
      "Loss at step 690: 0.128592\n",
      "Loss at step 700: 0.541096\n",
      "Loss at step 710: 0.21185\n",
      "Loss at step 720: 0.166192\n",
      "Loss at step 730: 0.147576\n",
      "Loss at step 740: 0.370184\n",
      "Loss at step 750: 0.20535\n",
      "Loss at step 760: 0.413414\n",
      "Loss at step 770: 0.425945\n",
      "Loss at step 780: 0.514647\n",
      "Loss at step 790: 0.735534\n",
      "Loss at step 800: 0.593749\n",
      "Loss at step 810: 0.465902\n",
      "Loss at step 820: 0.209527\n",
      "Loss at step 830: 0.42444\n",
      "Loss at step 840: 0.245934\n",
      "Loss at step 850: 0.168463\n",
      "Loss at step 860: 0.168853\n",
      "Loss at step 870: 0.146161\n",
      "Loss at step 880: 0.268932\n",
      "Loss at step 890: 0.273769\n",
      "Loss at step 900: 0.0997356\n",
      "Loss at step 910: 0.696824\n",
      "Loss at step 920: 0.306326\n",
      "Loss at step 930: 0.458407\n",
      "Loss at step 940: 0.17329\n",
      "Loss at step 950: 0.293895\n",
      "Loss at step 960: 0.141092\n",
      "Loss at step 970: 0.13954\n",
      "Loss at step 980: 0.0530283\n",
      "Loss at step 990: 0.398756\n",
      "Loss at step 1000: 0.416788\n",
      "Loss at step 1010: 0.44598\n",
      "Loss at step 1020: 0.221206\n",
      "Loss at step 1030: 0.262315\n",
      "Loss at step 1040: 0.1863\n",
      "Loss at step 1050: 0.192767\n",
      "Loss at step 1060: 0.165241\n",
      "Loss at step 1070: 0.214733\n",
      "Loss at step 1080: 0.472487\n",
      "Loss at step 1090: 0.325391\n",
      "Loss at step 1100: 0.316604\n",
      "Loss at step 1110: 0.0567078\n",
      "Loss at step 1120: 0.180835\n",
      "Loss at step 1130: 0.768561\n",
      "Loss at step 1140: 0.134383\n",
      "Loss at step 1150: 0.247842\n",
      "Loss at step 1160: 0.134849\n",
      "Loss at step 1170: 0.360924\n",
      "Loss at step 1180: 0.0515072\n",
      "Loss at step 1190: 0.223442\n",
      "Loss at step 1200: 0.167984\n",
      "Loss at step 1210: 0.396201\n",
      "Loss at step 1220: 0.331307\n",
      "Loss at step 1230: 0.443307\n",
      "Loss at step 1240: 0.421324\n",
      "Loss at step 1250: 0.721184\n",
      "Loss at step 1260: 0.220679\n",
      "Loss at step 1270: 0.10689\n",
      "Loss at step 1280: 0.161222\n",
      "Loss at step 1290: 0.039656\n",
      "Loss at step 1300: 0.22659\n",
      "Loss at step 1310: 0.147475\n",
      "Loss at step 1320: 0.131496\n",
      "Loss at step 1330: 0.438244\n",
      "Loss at step 1340: 0.448375\n",
      "Loss at step 1350: 0.107762\n",
      "Loss at step 1360: 0.233007\n",
      "Loss at step 1370: 0.127227\n",
      "Loss at step 1380: 0.3193\n",
      "Loss at step 1390: 0.105541\n",
      "Loss at step 1400: 0.187605\n",
      "Loss at step 1410: 0.174645\n",
      "Loss at step 1420: 0.126705\n",
      "Loss at step 1430: 0.0531103\n",
      "Loss at step 1440: 0.159304\n",
      "Loss at step 1450: 0.0604528\n",
      "Loss at step 1460: 0.182343\n",
      "Loss at step 1470: 0.316098\n",
      "Loss at step 1480: 0.0661604\n",
      "Loss at step 1490: 0.329714\n",
      "Loss at step 1500: 0.177123\n",
      "Loss at step 1510: 0.145806\n",
      "Loss at step 1520: 0.133244\n",
      "Loss at step 1530: 0.288014\n",
      "Loss at step 1540: 0.145484\n",
      "Loss at step 1550: 0.0861532\n",
      "Loss at step 1560: 0.04014\n",
      "Loss at step 1570: 0.0391926\n",
      "Loss at step 1580: 0.132203\n",
      "Loss at step 1590: 0.132463\n",
      "Loss at step 1600: 0.0184603\n",
      "Loss at step 1610: 0.116888\n",
      "Loss at step 1620: 0.053291\n",
      "Loss at step 1630: 0.181049\n",
      "Loss at step 1640: 0.505836\n",
      "Loss at step 1650: 0.166461\n",
      "Loss at step 1660: 0.0511348\n",
      "Loss at step 1670: 0.357182\n",
      "Loss at step 1680: 0.114191\n",
      "Loss at step 1690: 0.21175\n",
      "Loss at step 1700: 0.256478\n",
      "Loss at step 1710: 0.356558\n",
      "Loss at step 1720: 0.0537841\n",
      "Loss at step 1730: 0.210004\n",
      "Loss at step 1740: 0.301323\n",
      "Loss at step 1750: 0.0349992\n",
      "Loss at step 1760: 0.32342\n",
      "Loss at step 1770: 0.225347\n",
      "Loss at step 1780: 0.292691\n",
      "Loss at step 1790: 0.0992428\n",
      "Loss at step 1800: 0.0644144\n",
      "Loss at step 1810: 0.0899704\n",
      "Loss at step 1820: 0.0581671\n",
      "Loss at step 1830: 0.129381\n",
      "Loss at step 1840: 0.371572\n",
      "Loss at step 1850: 0.166879\n",
      "Loss at step 1860: 0.113063\n",
      "Loss at step 1870: 0.302877\n",
      "Loss at step 1880: 0.0425475\n",
      "Loss at step 1890: 0.0836104\n",
      "Loss at step 1900: 0.0300959\n",
      "Loss at step 1910: 0.423157\n",
      "Loss at step 1920: 0.0433975\n",
      "Loss at step 1930: 0.161926\n",
      "Loss at step 1940: 0.0599663\n",
      "Loss at step 1950: 0.0594043\n",
      "Loss at step 1960: 0.192087\n",
      "Loss at step 1970: 0.18133\n",
      "Loss at step 1980: 0.114538\n",
      "Loss at step 1990: 0.2141\n",
      "Loss at step 2000: 0.0338033\n",
      "Loss at step 2010: 0.263322\n",
      "Loss at step 2020: 0.0403399\n",
      "Loss at step 2030: 0.0659523\n",
      "Loss at step 2040: 0.213735\n",
      "Loss at step 2050: 0.0202718\n",
      "Loss at step 2060: 0.0302039\n",
      "Loss at step 2070: 0.0607653\n",
      "Loss at step 2080: 0.107031\n",
      "Loss at step 2090: 0.0342174\n",
      "Loss at step 2100: 0.17852\n",
      "Loss at step 2110: 0.208767\n",
      "Loss at step 2120: 0.187131\n",
      "Loss at step 2130: 0.10166\n",
      "Loss at step 2140: 0.0260898\n",
      "Loss at step 2150: 0.0492413\n",
      "Loss at step 2160: 0.282159\n",
      "Loss at step 2170: 0.317682\n",
      "Loss at step 2180: 0.0525334\n",
      "Loss at step 2190: 0.116889\n",
      "Loss at step 2200: 0.109766\n",
      "Loss at step 2210: 0.0985191\n",
      "Loss at step 2220: 0.15351\n",
      "Loss at step 2230: 0.449074\n",
      "Loss at step 2240: 0.0354354\n",
      "Loss at step 2250: 0.0341062\n",
      "Loss at step 2260: 0.0235957\n",
      "Loss at step 2270: 0.190289\n",
      "Loss at step 2280: 0.0342707\n",
      "Loss at step 2290: 0.204394\n",
      "Loss at step 2300: 0.338787\n",
      "Loss at step 2310: 0.035196\n",
      "Loss at step 2320: 0.25195\n",
      "Loss at step 2330: 0.71794\n",
      "Loss at step 2340: 0.0651479\n",
      "Loss at step 2350: 0.0254911\n",
      "Loss at step 2360: 0.0540418\n",
      "Loss at step 2370: 0.0675677\n",
      "Loss at step 2380: 0.179764\n",
      "Loss at step 2390: 0.0241324\n",
      "Loss at step 2400: 0.0339866\n",
      "Loss at step 2410: 0.204269\n",
      "Loss at step 2420: 0.225736\n",
      "Loss at step 2430: 0.424496\n",
      "Loss at step 2440: 0.0487414\n",
      "Loss at step 2450: 0.0569404\n",
      "Loss at step 2460: 0.0105229\n",
      "Loss at step 2470: 0.121186\n",
      "Loss at step 2480: 0.186262\n",
      "Loss at step 2490: 0.134404\n",
      "Loss at step 2500: 0.00513044\n",
      "Loss at step 2510: 0.0138544\n",
      "Loss at step 2520: 0.250625\n",
      "Loss at step 2530: 0.0106299\n",
      "Loss at step 2540: 0.0722969\n",
      "Loss at step 2550: 0.0676526\n",
      "Loss at step 2560: 0.257011\n",
      "Loss at step 2570: 0.349526\n",
      "Loss at step 2580: 0.0542118\n",
      "Loss at step 2590: 0.180637\n",
      "Loss at step 2600: 0.0768567\n",
      "Loss at step 2610: 0.00631468\n",
      "Loss at step 2620: 0.0175676\n",
      "Loss at step 2630: 0.114643\n",
      "Loss at step 2640: 0.120794\n",
      "Loss at step 2650: 0.199022\n",
      "Loss at step 2660: 0.122689\n",
      "Loss at step 2670: 0.0304363\n",
      "Loss at step 2680: 0.209485\n",
      "Loss at step 2690: 0.108979\n",
      "Loss at step 2700: 0.0314712\n",
      "Loss at step 2710: 0.0725359\n",
      "Loss at step 2720: 0.0098222\n",
      "Loss at step 2730: 0.0103948\n",
      "Loss at step 2740: 0.282597\n",
      "Loss at step 2750: 0.0443358\n",
      "Loss at step 2760: 0.216044\n",
      "Loss at step 2770: 0.0405401\n",
      "Loss at step 2780: 0.0380009\n",
      "Loss at step 2790: 0.0464544\n",
      "Loss at step 2800: 0.013116\n",
      "Loss at step 2810: 0.149185\n",
      "Loss at step 2820: 0.143317\n",
      "Loss at step 2830: 0.200732\n",
      "Loss at step 2840: 0.16823\n",
      "Loss at step 2850: 0.133776\n",
      "Loss at step 2860: 0.0171141\n",
      "Loss at step 2870: 0.187514\n",
      "Loss at step 2880: 0.0630079\n",
      "Loss at step 2890: 0.142589\n",
      "Loss at step 2900: 0.138459\n",
      "Loss at step 2910: 0.292732\n",
      "Loss at step 2920: 0.17466\n",
      "Loss at step 2930: 0.175865\n",
      "Loss at step 2940: 0.220864\n",
      "Loss at step 2950: 0.194748\n",
      "Loss at step 2960: 0.159794\n",
      "Loss at step 2970: 0.250263\n",
      "Loss at step 2980: 0.0276365\n",
      "Loss at step 2990: 0.0327257\n",
      "Loss at step 3000: 0.0222844\n",
      "Loss at step 3010: 0.238578\n",
      "Loss at step 3020: 0.0480388\n",
      "Loss at step 3030: 0.0105088\n",
      "Loss at step 3040: 0.168521\n",
      "Loss at step 3050: 0.0107647\n",
      "Loss at step 3060: 0.167847\n",
      "Loss at step 3070: 0.0661752\n",
      "Loss at step 3080: 0.0104656\n",
      "Loss at step 3090: 0.0201982\n",
      "Loss at step 3100: 0.0402186\n",
      "Loss at step 3110: 0.0385502\n",
      "Loss at step 3120: 0.203524\n",
      "Loss at step 3130: 0.0572669\n",
      "Loss at step 3140: 0.00587907\n",
      "Loss at step 3150: 0.034266\n",
      "Loss at step 3160: 0.0950095\n",
      "Loss at step 3170: 0.494844\n",
      "Loss at step 3180: 0.0122759\n",
      "Loss at step 3190: 0.095317\n",
      "Loss at step 3200: 0.276693\n",
      "Loss at step 3210: 0.158755\n",
      "Loss at step 3220: 0.655775\n",
      "Loss at step 3230: 0.0471989\n",
      "Loss at step 3240: 0.0189257\n",
      "Loss at step 3250: 0.0266125\n",
      "Loss at step 3260: 0.00473224\n",
      "Loss at step 3270: 0.176361\n",
      "Loss at step 3280: 0.00159922\n",
      "Loss at step 3290: 0.159027\n",
      "Loss at step 3300: 0.25342\n",
      "Loss at step 3310: 0.222207\n",
      "Loss at step 3320: 0.0218596\n",
      "Loss at step 3330: 0.12357\n",
      "Loss at step 3340: 0.0229898\n",
      "Loss at step 3350: 0.0780552\n",
      "Loss at step 3360: 0.0799358\n",
      "Loss at step 3370: 0.230649\n",
      "Loss at step 3380: 0.0779984\n",
      "Loss at step 3390: 0.0146968\n",
      "Loss at step 3400: 0.368721\n",
      "Loss at step 3410: 0.0207223\n",
      "Loss at step 3420: 0.397588\n",
      "Loss at step 3430: 0.323374\n",
      "Loss at step 3440: 0.0338185\n",
      "Loss at step 3450: 0.145048\n",
      "Loss at step 3460: 0.375276\n",
      "Loss at step 3470: 0.106244\n",
      "Loss at step 3480: 0.0235857\n",
      "Loss at step 3490: 0.0583231\n",
      "Loss at step 3500: 0.0850857\n",
      "Loss at step 3510: 0.0146372\n",
      "Loss at step 3520: 0.0575331\n",
      "Loss at step 3530: 0.468503\n",
      "Loss at step 3540: 0.0589793\n",
      "Loss at step 3550: 0.0144748\n",
      "Loss at step 3560: 0.106432\n",
      "Loss at step 3570: 0.31985\n",
      "Loss at step 3580: 0.175094\n",
      "Loss at step 3590: 0.307366\n",
      "Loss at step 3600: 0.0530027\n",
      "Loss at step 3610: 0.0230524\n",
      "Loss at step 3620: 0.107877\n",
      "Loss at step 3630: 0.081888\n",
      "Loss at step 3640: 0.00996992\n",
      "Loss at step 3650: 0.00355368\n",
      "Loss at step 3660: 0.092541\n",
      "Loss at step 3670: 0.0262822\n",
      "Loss at step 3680: 0.0182297\n",
      "Loss at step 3690: 0.0511576\n",
      "Loss at step 3700: 0.00054216\n",
      "Loss at step 3710: 0.00878359\n",
      "Loss at step 3720: 0.0518125\n",
      "Loss at step 3730: 0.0379368\n",
      "Loss at step 3740: 0.0141177\n",
      "Loss at step 3750: 0.0564714\n",
      "Loss at step 3760: 0.0404081\n",
      "Loss at step 3770: 0.0663237\n",
      "Loss at step 3780: 0.0457476\n",
      "Loss at step 3790: 0.0583928\n",
      "Loss at step 3800: 0.00975688\n",
      "Loss at step 3810: 0.0380678\n",
      "Loss at step 3820: 0.161584\n",
      "Loss at step 3830: 0.158004\n",
      "Loss at step 3840: 0.0592418\n",
      "Loss at step 3850: 0.0884399\n",
      "Loss at step 3860: 0.0653767\n",
      "Loss at step 3870: 0.0138391\n",
      "Loss at step 3880: 0.0322114\n",
      "Loss at step 3890: 0.31405\n",
      "Loss at step 3900: 0.12335\n",
      "Loss at step 3910: 0.0118055\n",
      "Loss at step 3920: 0.0147243\n",
      "Loss at step 3930: 0.0467136\n",
      "Loss at step 3940: 0.0690747\n",
      "Loss at step 3950: 0.0512844\n",
      "Loss at step 3960: 0.0499358\n",
      "Loss at step 3970: 0.00900374\n",
      "Loss at step 3980: 0.0156595\n",
      "Loss at step 3990: 0.0680066\n",
      "Loss at step 4000: 0.00241336\n",
      "Loss at step 4010: 0.032583\n",
      "Loss at step 4020: 0.119729\n",
      "Loss at step 4030: 0.0234899\n",
      "Loss at step 4040: 0.111304\n",
      "Loss at step 4050: 0.366068\n",
      "Loss at step 4060: 0.0821554\n",
      "Loss at step 4070: 0.105317\n",
      "Loss at step 4080: 0.0227904\n",
      "Loss at step 4090: 0.269988\n",
      "Loss at step 4100: 0.0505989\n",
      "Loss at step 4110: 0.0611362\n",
      "Loss at step 4120: 0.0292688\n",
      "Loss at step 4130: 0.0355231\n",
      "Loss at step 4140: 0.030063\n",
      "Loss at step 4150: 0.00645752\n",
      "Loss at step 4160: 0.070034\n",
      "Loss at step 4170: 0.0325586\n",
      "Loss at step 4180: 0.0102738\n",
      "Loss at step 4190: 0.134702\n",
      "Loss at step 4200: 0.163962\n",
      "Loss at step 4210: 0.0998637\n",
      "Loss at step 4220: 0.421553\n",
      "Loss at step 4230: 0.0720087\n",
      "Loss at step 4240: 0.023964\n",
      "Loss at step 4250: 0.116303\n",
      "Loss at step 4260: 0.112992\n",
      "Loss at step 4270: 0.958348\n",
      "Loss at step 4280: 0.0307533\n",
      "Loss at step 4290: 0.0828121\n",
      "Loss at step 4300: 0.00517888\n",
      "Loss at step 4310: 0.0461506\n",
      "Loss at step 4320: 0.08221\n",
      "Loss at step 4330: 0.0126088\n",
      "Loss at step 4340: 0.0514763\n",
      "Loss at step 4350: 0.14757\n",
      "Loss at step 4360: 0.010898\n",
      "Loss at step 4370: 0.0232033\n",
      "Loss at step 4380: 0.00678082\n",
      "Loss at step 4390: 0.0220894\n",
      "Loss at step 4400: 0.0142526\n",
      "Loss at step 4410: 0.00431316\n",
      "Loss at step 4420: 0.0544382\n",
      "Loss at step 4430: 0.000678036\n",
      "Loss at step 4440: 0.0189898\n",
      "Loss at step 4450: 0.113895\n",
      "Loss at step 4460: 0.0631721\n",
      "Loss at step 4470: 0.200523\n",
      "Loss at step 4480: 0.0137706\n",
      "Loss at step 4490: 0.00919697\n",
      "Loss at step 4500: 0.00669502\n",
      "Loss at step 4510: 0.384923\n",
      "Loss at step 4520: 0.122897\n",
      "Loss at step 4530: 0.0683651\n",
      "Loss at step 4540: 0.0443367\n",
      "Loss at step 4550: 0.0780911\n",
      "Loss at step 4560: 0.0016064\n",
      "Loss at step 4570: 0.0044582\n",
      "Loss at step 4580: 0.0812832\n",
      "Loss at step 4590: 0.219703\n",
      "Loss at step 4600: 0.00272635\n",
      "Loss at step 4610: 0.0130296\n",
      "Loss at step 4620: 0.166441\n",
      "Loss at step 4630: 0.281943\n",
      "Loss at step 4640: 0.434115\n",
      "Loss at step 4650: 0.0253709\n",
      "Loss at step 4660: 0.0514285\n",
      "Loss at step 4670: 0.223361\n",
      "Loss at step 4680: 0.025825\n",
      "Loss at step 4690: 0.025489\n",
      "Loss at step 4700: 0.060374\n",
      "Loss at step 4710: 0.287697\n",
      "Loss at step 4720: 0.00996204\n",
      "Loss at step 4730: 0.0611453\n",
      "Loss at step 4740: 0.00526487\n",
      "Loss at step 4750: 0.192405\n",
      "Loss at step 4760: 0.0424253\n",
      "Loss at step 4770: 0.368069\n",
      "Loss at step 4780: 0.0542363\n",
      "Loss at step 4790: 0.0460014\n",
      "Loss at step 4800: 0.00137706\n",
      "Loss at step 4810: 0.00775982\n",
      "Loss at step 4820: 0.0842988\n",
      "Loss at step 4830: 0.0646517\n",
      "Loss at step 4840: 0.0861277\n",
      "Loss at step 4850: 0.0129906\n",
      "Loss at step 4860: 0.0722715\n",
      "Loss at step 4870: 0.00218765\n",
      "Loss at step 4880: 0.0361851\n",
      "Loss at step 4890: 0.0624075\n",
      "Loss at step 4900: 0.21962\n",
      "Loss at step 4910: 0.100798\n",
      "Loss at step 4920: 0.067426\n",
      "Loss at step 4930: 0.0404143\n",
      "Loss at step 4940: 0.0627086\n",
      "Loss at step 4950: 0.0297524\n",
      "Loss at step 4960: 0.0201081\n",
      "Loss at step 4970: 0.398388\n",
      "Loss at step 4980: 0.00240532\n",
      "Loss at step 4990: 0.100902\n",
      "Loss at step 5000: 0.203853\n",
      "Loss at step 5010: 0.00892796\n",
      "Loss at step 5020: 0.00253634\n",
      "Loss at step 5030: 0.0332439\n",
      "Loss at step 5040: 0.236997\n",
      "Loss at step 5050: 0.279022\n",
      "Loss at step 5060: 0.00491387\n",
      "Loss at step 5070: 0.14882\n",
      "Loss at step 5080: 0.0109247\n",
      "Loss at step 5090: 0.0870554\n",
      "Loss at step 5100: 0.00319488\n",
      "Loss at step 5110: 0.0262371\n",
      "Loss at step 5120: 0.162285\n",
      "Loss at step 5130: 0.0243227\n",
      "Loss at step 5140: 0.483702\n",
      "Loss at step 5150: 0.0285561\n",
      "Loss at step 5160: 0.0185444\n",
      "Loss at step 5170: 0.00797453\n",
      "Loss at step 5180: 0.0327592\n",
      "Loss at step 5190: 0.00692055\n",
      "Loss at step 5200: 0.0672\n",
      "Loss at step 5210: 0.00938452\n",
      "Loss at step 5220: 0.0885592\n",
      "Loss at step 5230: 0.000434268\n",
      "Loss at step 5240: 0.0246013\n",
      "Loss at step 5250: 0.0357646\n",
      "Loss at step 5260: 0.32567\n",
      "Loss at step 5270: 0.00885837\n",
      "Loss at step 5280: 0.0805094\n",
      "Loss at step 5290: 0.00142331\n",
      "Loss at step 5300: 0.115688\n",
      "Loss at step 5310: 0.00523044\n",
      "Loss at step 5320: 0.00847388\n",
      "Loss at step 5330: 0.0213766\n",
      "Loss at step 5340: 0.0882589\n",
      "Loss at step 5350: 0.0637948\n",
      "Loss at step 5360: 0.193929\n",
      "Loss at step 5370: 0.00220524\n",
      "Loss at step 5380: 0.0130474\n",
      "Loss at step 5390: 0.0786873\n",
      "Loss at step 5400: 0.00804188\n",
      "Loss at step 5410: 0.101919\n",
      "Loss at step 5420: 0.0257656\n",
      "Loss at step 5430: 0.00889728\n",
      "Loss at step 5440: 0.265628\n",
      "Loss at step 5450: 0.023695\n",
      "Loss at step 5460: 0.0682037\n",
      "Loss at step 5470: 0.0142823\n",
      "Loss at step 5480: 0.0131685\n",
      "Loss at step 5490: 0.0301952\n",
      "Loss at step 5500: 0.21517\n",
      "Loss at step 5510: 0.00982272\n",
      "Loss at step 5520: 0.0323659\n",
      "Loss at step 5530: 0.183918\n",
      "Loss at step 5540: 0.0272833\n",
      "Loss at step 5550: 0.0730983\n",
      "Loss at step 5560: 0.0230051\n",
      "Loss at step 5570: 0.00386847\n",
      "Loss at step 5580: 0.112541\n",
      "Loss at step 5590: 0.00275289\n",
      "Loss at step 5600: 0.0445025\n",
      "Loss at step 5610: 0.0198154\n",
      "Loss at step 5620: 0.0141569\n",
      "Loss at step 5630: 0.205483\n",
      "Loss at step 5640: 0.00619136\n",
      "Loss at step 5650: 0.00395856\n",
      "Loss at step 5660: 0.0159057\n",
      "Loss at step 5670: 0.0205614\n",
      "Loss at step 5680: 0.00229754\n",
      "Loss at step 5690: 0.00463752\n",
      "Loss at step 5700: 0.00985814\n",
      "Loss at step 5710: 0.0167296\n",
      "Loss at step 5720: 0.0682999\n",
      "Loss at step 5730: 0.0104427\n",
      "Loss at step 5740: 0.0162181\n",
      "Loss at step 5750: 0.0280016\n",
      "Loss at step 5760: 0.00366005\n",
      "Loss at step 5770: 0.00116727\n",
      "Loss at step 5780: 0.119645\n",
      "Loss at step 5790: 0.108599\n",
      "Loss at step 5800: 0.134676\n",
      "Loss at step 5810: 0.229024\n",
      "Loss at step 5820: 0.0877353\n",
      "Loss at step 5830: 0.0365385\n",
      "Loss at step 5840: 0.00929152\n",
      "Loss at step 5850: 0.0134428\n",
      "Loss at step 5860: 0.0500269\n",
      "Loss at step 5870: 0.0291072\n",
      "Loss at step 5880: 0.0439836\n",
      "Loss at step 5890: 0.0803179\n",
      "Loss at step 5900: 0.049301\n",
      "Loss at step 5910: 0.00880204\n",
      "Loss at step 5920: 0.155514\n",
      "Loss at step 5930: 0.0888276\n",
      "Loss at step 5940: 0.00255093\n",
      "Loss at step 5950: 0.0579444\n",
      "Loss at step 5960: 0.130204\n",
      "Loss at step 5970: 0.145119\n",
      "Loss at step 5980: 0.00211128\n",
      "Loss at step 5990: 0.0766978\n",
      "Loss at step 6000: 0.00134491\n",
      "Loss at step 6010: 0.00905436\n",
      "Loss at step 6020: 0.0131438\n",
      "Loss at step 6030: 0.0868028\n",
      "Loss at step 6040: 0.0206161\n",
      "Loss at step 6050: 0.0054858\n",
      "Loss at step 6060: 0.013645\n",
      "Loss at step 6070: 0.0233126\n",
      "Loss at step 6080: 0.000631442\n",
      "Loss at step 6090: 0.0275734\n",
      "Loss at step 6100: 0.0230548\n",
      "Loss at step 6110: 0.00310673\n",
      "Loss at step 6120: 0.10372\n",
      "Loss at step 6130: 0.00172749\n",
      "Loss at step 6140: 0.000563375\n",
      "Loss at step 6150: 0.00483656\n",
      "Loss at step 6160: 0.138136\n",
      "Loss at step 6170: 0.00485196\n",
      "Loss at step 6180: 0.00102042\n",
      "Loss at step 6190: 0.178531\n",
      "Loss at step 6200: 0.000659977\n",
      "Loss at step 6210: 0.0131178\n",
      "Loss at step 6220: 0.0863579\n",
      "Loss at step 6230: 0.0128276\n",
      "Loss at step 6240: 0.0123943\n",
      "Loss at step 6250: 0.00167572\n",
      "Loss at step 6260: 0.0268427\n",
      "Loss at step 6270: 0.00485947\n",
      "Loss at step 6280: 0.248869\n",
      "Loss at step 6290: 0.0295389\n",
      "Loss at step 6300: 0.0525409\n",
      "Loss at step 6310: 0.00454473\n",
      "Loss at step 6320: 0.012439\n",
      "Loss at step 6330: 0.142328\n",
      "Loss at step 6340: 0.0737004\n",
      "Loss at step 6350: 0.038655\n",
      "Loss at step 6360: 0.0560669\n",
      "Loss at step 6370: 0.0259531\n",
      "Loss at step 6380: 0.00740007\n",
      "Loss at step 6390: 0.0738997\n",
      "Loss at step 6400: 0.438528\n",
      "Loss at step 6410: 0.019486\n",
      "Loss at step 6420: 0.0253899\n",
      "Loss at step 6430: 0.04114\n",
      "Loss at step 6440: 0.0389719\n",
      "Loss at step 6450: 0.0295572\n",
      "Loss at step 6460: 0.126523\n",
      "Loss at step 6470: 0.0111725\n",
      "Loss at step 6480: 0.0370656\n",
      "Loss at step 6490: 0.0493366\n",
      "Loss at step 6500: 0.00346641\n",
      "Loss at step 6510: 0.0147562\n",
      "Loss at step 6520: 0.000958154\n",
      "Loss at step 6530: 0.0754014\n",
      "Loss at step 6540: 0.00765197\n",
      "Loss at step 6550: 0.0126917\n",
      "Loss at step 6560: 0.383789\n",
      "Loss at step 6570: 0.0297174\n",
      "Loss at step 6580: 0.430779\n",
      "Loss at step 6590: 0.00608724\n",
      "Loss at step 6600: 0.297336\n",
      "Loss at step 6610: 0.0365241\n",
      "Loss at step 6620: 0.0457756\n",
      "Loss at step 6630: 0.0123148\n",
      "Loss at step 6640: 0.00436512\n",
      "Loss at step 6650: 0.00152125\n",
      "Loss at step 6660: 0.00474235\n",
      "Loss at step 6670: 0.0531699\n",
      "Loss at step 6680: 0.00578839\n",
      "Loss at step 6690: 0.00297242\n",
      "Loss at step 6700: 0.000893958\n",
      "Loss at step 6710: 0.155241\n",
      "Loss at step 6720: 0.00142592\n",
      "Loss at step 6730: 0.141574\n",
      "Loss at step 6740: 0.000866526\n",
      "Loss at step 6750: 0.03617\n",
      "Loss at step 6760: 0.0191537\n",
      "Loss at step 6770: 0.223309\n",
      "Loss at step 6780: 0.0193882\n",
      "Loss at step 6790: 0.334575\n",
      "Loss at step 6800: 0.0284082\n",
      "Loss at step 6810: 0.00879986\n",
      "Loss at step 6820: 0.000226548\n",
      "Loss at step 6830: 0.011612\n",
      "Loss at step 6840: 0.0121279\n",
      "Loss at step 6850: 0.0228368\n",
      "Loss at step 6860: 0.02012\n",
      "Loss at step 6870: 0.647889\n",
      "Loss at step 6880: 0.0226799\n",
      "Loss at step 6890: 0.00573104\n",
      "Loss at step 6900: 0.00562533\n",
      "Loss at step 6910: 0.0977918\n",
      "Loss at step 6920: 0.137391\n",
      "Loss at step 6930: 0.00138135\n",
      "Loss at step 6940: 0.152662\n",
      "Loss at step 6950: 0.0252894\n",
      "Loss at step 6960: 0.26071\n",
      "Loss at step 6970: 0.0554385\n",
      "Loss at step 6980: 0.0100225\n",
      "Loss at step 6990: 0.047995\n",
      "Loss at step 7000: 0.234616\n",
      "Loss at step 7010: 0.0519213\n",
      "Loss at step 7020: 0.0055725\n",
      "Loss at step 7030: 0.293619\n",
      "Loss at step 7040: 0.233676\n",
      "Loss at step 7050: 0.128168\n",
      "Loss at step 7060: 0.213589\n",
      "Loss at step 7070: 0.220791\n",
      "Loss at step 7080: 0.00207434\n",
      "Loss at step 7090: 0.253879\n",
      "Loss at step 7100: 0.0222448\n",
      "Loss at step 7110: 0.000961784\n",
      "Loss at step 7120: 0.00532907\n",
      "Loss at step 7130: 0.223314\n",
      "Loss at step 7140: 0.0122696\n",
      "Loss at step 7150: 0.0731566\n",
      "Loss at step 7160: 0.00422205\n",
      "Loss at step 7170: 0.0481315\n",
      "Loss at step 7180: 0.22686\n",
      "Loss at step 7190: 0.140948\n",
      "Loss at step 7200: 0.0673473\n",
      "Loss at step 7210: 0.0657088\n",
      "Loss at step 7220: 0.0955729\n",
      "Loss at step 7230: 0.00993856\n",
      "Loss at step 7240: 0.00253634\n",
      "Loss at step 7250: 0.0336689\n",
      "Loss at step 7260: 0.00217595\n",
      "Loss at step 7270: 0.143544\n",
      "Loss at step 7280: 0.150167\n",
      "Loss at step 7290: 0.0494963\n",
      "Loss at step 7300: 0.0500605\n",
      "Loss at step 7310: 0.0224362\n",
      "Loss at step 7320: 0.00591196\n",
      "Loss at step 7330: 0.00378711\n",
      "Loss at step 7340: 0.0479365\n",
      "Loss at step 7350: 0.00605757\n",
      "Loss at step 7360: 0.0421863\n",
      "Loss at step 7370: 0.00246654\n",
      "Loss at step 7380: 0.0712196\n",
      "Loss at step 7390: 0.00473924\n",
      "Loss at step 7400: 0.0213518\n",
      "Loss at step 7410: 0.0932388\n",
      "Loss at step 7420: 0.00319175\n",
      "Loss at step 7430: 0.00213834\n",
      "Loss at step 7440: 0.00471463\n",
      "Loss at step 7450: 0.00418086\n",
      "Loss at step 7460: 0.00244161\n",
      "Loss at step 7470: 0.113704\n",
      "Loss at step 7480: 0.00389476\n",
      "Loss at step 7490: 0.120809\n",
      "Loss at step 7500: 0.0158854\n",
      "Loss at step 7510: 0.0229773\n",
      "Loss at step 7520: 0.000627253\n",
      "Loss at step 7530: 0.00656163\n",
      "Loss at step 7540: 0.0328656\n",
      "Loss at step 7550: 0.0111461\n",
      "Loss at step 7560: 0.00425429\n",
      "Loss at step 7570: 0.004078\n",
      "Loss at step 7580: 0.178913\n",
      "Loss at step 7590: 0.0359794\n",
      "Loss at step 7600: 0.213828\n",
      "Loss at step 7610: 0.0076447\n",
      "Loss at step 7620: 0.270159\n",
      "Loss at step 7630: 0.111618\n",
      "Loss at step 7640: 0.00807705\n",
      "Loss at step 7650: 0.159823\n",
      "Loss at step 7660: 0.0309162\n",
      "Loss at step 7670: 0.000566139\n",
      "Loss at step 7680: 0.246346\n",
      "Loss at step 7690: 0.035497\n",
      "Loss at step 7700: 0.0145299\n",
      "Loss at step 7710: 0.0748948\n",
      "Loss at step 7720: 0.140617\n",
      "Loss at step 7730: 0.000575894\n",
      "Loss at step 7740: 0.0613929\n",
      "Loss at step 7750: 0.000466851\n",
      "Loss at step 7760: 0.0313299\n",
      "Loss at step 7770: 0.00457013\n",
      "Loss at step 7780: 0.0122646\n",
      "Loss at step 7790: 0.0235487\n",
      "Loss at step 7800: 0.000902563\n",
      "Loss at step 7810: 0.0104584\n",
      "Loss at step 7820: 0.0175605\n",
      "Loss at step 7830: 0.0104033\n",
      "Loss at step 7840: 0.0789088\n",
      "Loss at step 7850: 0.0227188\n",
      "Loss at step 7860: 0.0390284\n",
      "Loss at step 7870: 0.02888\n",
      "Loss at step 7880: 0.227868\n",
      "Loss at step 7890: 0.0708569\n",
      "Loss at step 7900: 0.170023\n",
      "Loss at step 7910: 0.0535325\n",
      "Loss at step 7920: 0.00353054\n",
      "Loss at step 7930: 0.0116016\n",
      "Loss at step 7940: 0.126407\n",
      "Loss at step 7950: 0.0136058\n",
      "Loss at step 7960: 0.0565544\n",
      "Loss at step 7970: 0.00893284\n",
      "Loss at step 7980: 0.00312985\n",
      "Loss at step 7990: 0.00242156\n"
     ]
    }
   ],
   "source": [
    "clf.train(Xtrain,ytrain,n_iters=8000,batch_size=20,learning_rate=7e-4,keep_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97988093, 0.071598686]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for training data\n",
    "clf.score(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97690475, 0.079388298]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy and loss for validation data\n",
    "clf.score(Xval,yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss function and accuracy plots visualized using tensorboard with logdir=convClf_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
